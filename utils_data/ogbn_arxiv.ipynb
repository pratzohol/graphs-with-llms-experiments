{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/prateek/graphs-with-llms-experiments/utils_data', '/home/prateek/miniconda3/envs/torch_pyg/lib/python310.zip', '/home/prateek/miniconda3/envs/torch_pyg/lib/python3.10', '/home/prateek/miniconda3/envs/torch_pyg/lib/python3.10/lib-dynload', '', '/home/prateek/miniconda3/envs/torch_pyg/lib/python3.10/site-packages', '/home/prateek/graphs-with-llms-experiments']\n"
     ]
    }
   ],
   "source": [
    "from ogb.nodeproppred import PygNodePropPredDataset\n",
    "import os\n",
    "import os.path as osp\n",
    "import sys\n",
    "import torch_geometric\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch_geometric.data import InMemoryDataset, download_url\n",
    "\n",
    "sys.path.append(osp.abspath(\"..\"))\n",
    "print(sys.path)\n",
    "\n",
    "from utils.encoder import SentenceEncoder\n",
    "from utils_data.custom_pyg import CustomPygDataset\n",
    "from utils.dataloader import GetDataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the ogbn-arxiv dataset\n",
    "data_root = \"../data\"\n",
    "arxiv = PygNodePropPredDataset(name='ogbn-arxiv', root=data_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(num_nodes=169343, edge_index=[2, 1166243], x=[169343, 128], node_year=[169343, 1], y=[169343, 1])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = arxiv[0]\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_taxonomy(data_root):\n",
    "    # read categories and description file\n",
    "    f = open(osp.join(data_root, \"ogbn_arxiv\", \"arxiv_CS_categories.txt\"), \"r\").readlines()\n",
    "\n",
    "    state = 0\n",
    "    result = {\"id\": [], \"name\": [], \"description\": []}\n",
    "\n",
    "    for line in f:\n",
    "        if state == 0:\n",
    "            assert line.strip().startswith(\"cs.\")\n",
    "            category = (\"arxiv \"\n",
    "                + \" \".join(line.strip().split(\" \")[0].split(\".\")).lower())\n",
    "            # e.g. cs lo\n",
    "\n",
    "            name = line.strip()[7:-1]  # e. g. Logic in CS\n",
    "            result[\"id\"].append(category)\n",
    "            result[\"name\"].append(name)\n",
    "            state = 1\n",
    "            continue\n",
    "\n",
    "        elif state == 1:\n",
    "            description = line.strip()\n",
    "            result[\"description\"].append(description)\n",
    "            state = 2\n",
    "            continue\n",
    "\n",
    "        elif state == 2:\n",
    "            state = 0\n",
    "            continue\n",
    "\n",
    "    arxiv_cs_taxonomy = pd.DataFrame(result)\n",
    "    return arxiv_cs_taxonomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_feature(data_root):\n",
    "    nodeidx2paperid = pd.read_csv(osp.join(data_root, \"ogbn_arxiv/mapping/nodeidx2paperid.csv.gz\"), index_col=\"node idx\")\n",
    "\n",
    "    # Load the title and abstract of each paper\n",
    "    titleabs_url = \"https://snap.stanford.edu/ogb/data/misc/ogbn_arxiv/titleabs.tsv\"\n",
    "    titleabs_path = osp.join(data_root, \"ogbn_arxiv\", \"titleabs.tsv\")\n",
    "\n",
    "    if (osp.exists(titleabs_path)):\n",
    "        titleabs = pd.read_csv(titleabs_path, sep=\"\\t\", names=[\"paper id\", \"title\", \"abstract\"], index_col=\"paper id\")\n",
    "    else:\n",
    "        titleabs = pd.read_csv(titleabs_url, sep=\"\\t\", names=[\"paper id\", \"title\", \"abstract\"], index_col=\"paper id\")\n",
    "\n",
    "    titleabs = nodeidx2paperid.join(titleabs, on=\"paper id\")\n",
    "\n",
    "\n",
    "    # Prompt for the feature of nodes (can be changed accordingly)\n",
    "    node_feature_prompt = (\"Feature Node.\\n\"\n",
    "                        + \"Paper Title and Abstract : \"\n",
    "                        + titleabs[\"title\"]\n",
    "                        + \" + \"\n",
    "                        + titleabs[\"abstract\"])\n",
    "\n",
    "    node_feature_prompt_list = node_feature_prompt.values\n",
    "    return node_feature_prompt_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_feature(data_root):\n",
    "    arxiv_cs_taxonomy = get_taxonomy(data_root)\n",
    "\n",
    "    mapping_file = osp.join(data_root, \"ogbn_arxiv\", \"mapping\", \"labelidx2arxivcategeory.csv.gz\")\n",
    "    labelidx2arxivcategory = pd.read_csv(mapping_file)\n",
    "\n",
    "    arxiv_categ_vals = pd.merge(labelidx2arxivcategory, arxiv_cs_taxonomy, left_on=\"arxiv category\", right_on=\"id\")\n",
    "\n",
    "\n",
    "    # Prompt for the label nodes (can be changed accordingly)\n",
    "    label_node_prompt = (\"Prompt Node.\\n\"\n",
    "                        + \"Literature Category and Description: \"\n",
    "                        + arxiv_categ_vals[\"name\"]\n",
    "                        + \" + \"\n",
    "                        + arxiv_categ_vals[\"description\"])\n",
    "\n",
    "    label_node_prompt_list = label_node_prompt.values\n",
    "    return label_node_prompt_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArxivPyGDataset(InMemoryDataset):\n",
    "    def __init__(self, dataRoot=\"../data\", custom_dataRoot=\"../custom_data\", sentence_encoder=None, transform=None, pre_transform=None, pre_filter=None):\n",
    "        self.data_root = dataRoot\n",
    "        self.custom_data_root = custom_dataRoot\n",
    "        self.sentence_encoder = sentence_encoder\n",
    "        self.custom_data_dir = osp.join(self.custom_data_root, f\"ogbn_arxiv_{self.sentence_encoder.name}\")\n",
    "\n",
    "        if not osp.exists(self.custom_data_dir):\n",
    "            os.makedirs(self.custom_data_dir)\n",
    "\n",
    "        super().__init__(self.custom_data_dir, transform, pre_transform, pre_filter)\n",
    "\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return []\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return [\"data.pt\", \"texts.pkl\"]\n",
    "\n",
    "    def text_to_embed(self, texts):\n",
    "        if self.sentence_encoder is None:\n",
    "            raise NotImplementedError(\"Sentence Encoder is not passed\")\n",
    "        if texts is None:\n",
    "            return None\n",
    "        else:\n",
    "            return self.sentence_encoder.encode(texts) # returns to self.device\n",
    "\n",
    "    def encode_texts(self, texts):\n",
    "        if isinstance(texts[0], str):\n",
    "            return self.text_to_embed(texts)\n",
    "        return [self.text_to_embed(t) for t in texts]\n",
    "\n",
    "    def generate_custom_data(self):\n",
    "        node_texts = get_node_feature(self.data_root).tolist()\n",
    "        label_texts = get_label_feature(self.data_root).tolist()\n",
    "\n",
    "        # Prompt for prompt node/edge and edge texts (can be changed accordingly)\n",
    "        edge_texts = [\"Feature Edge.\\n Citation\"]\n",
    "        prompt_texts = [\"Prompt Node.\\n Node Classification of Literature Category\"]\n",
    "        prompt_edge_texts = [\"Prompt Edge.\"]\n",
    "\n",
    "        return [node_texts, label_texts, edge_texts, prompt_texts, prompt_edge_texts]\n",
    "\n",
    "    def process(self):\n",
    "        arxiv_data = PygNodePropPredDataset(name=\"ogbn-arxiv\", root=self.data_root)\n",
    "        arxiv_data_list = arxiv_data._data\n",
    "        arxiv_data_list.y = arxiv_data_list.y.squeeze()  # to flatten the y tensor\n",
    "\n",
    "        texts = self.generate_custom_data()\n",
    "        texts_embed = self.encode_texts(texts)\n",
    "\n",
    "        torch.save(texts, self.processed_paths[1])\n",
    "\n",
    "        arxiv_data_list.x_text_feat = texts_embed[0] # node text feature\n",
    "        arxiv_data_list.label_text_feat = texts_embed[1] # label text feature\n",
    "        arxiv_data_list.edge_text_feat = texts_embed[2] # edge text feature\n",
    "        arxiv_data_list.prompt_text_feat = texts_embed[3] # prompt node text feature\n",
    "        arxiv_data_list.prompt_edge_feat = texts_embed[4] # prompt edge text feature\n",
    "\n",
    "        # get dataset split\n",
    "        split_idx = arxiv_data.get_idx_split()\n",
    "        train_idx, valid_idx, test_idx = split_idx[\"train\"], split_idx[\"valid\"], split_idx[\"test\"]\n",
    "\n",
    "        arxiv_data_list.train_mask = train_idx\n",
    "        arxiv_data_list.val_mask = valid_idx\n",
    "        arxiv_data_list.test_mask = test_idx\n",
    "\n",
    "        data, slices = self.collate([arxiv_data_list]) # Pass the data_list as a list\n",
    "\n",
    "        torch.save((data, slices), self.processed_paths[0])\n",
    "        print(\"Arxiv is processed. Saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prateek/miniconda3/envs/torch_pyg/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "LMencoder = SentenceEncoder(root=\"../lang_models\", name=\"ST\", device=1)\n",
    "custom_arxiv = ArxivPyGDataset(dataRoot=data_root, sentence_encoder=LMencoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "LMencoder2 = SentenceEncoder(root=\"../lang_models\", name=\"roberta\", device=1)\n",
    "custom_arxiv2 = ArxivPyGDataset(dataRoot=data_root, sentence_encoder=LMencoder2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(num_nodes=169343, edge_index=[2, 1166243], x=[169343, 128], node_year=[169343, 1], y=[169343], x_text_feat=[169343, 768], label_text_feat=[40, 768], edge_text_feat=[1, 768], prompt_text_feat=[1, 768], prompt_edge_feat=[1, 768], train_mask=[90941], val_mask=[29799], test_mask=[48603])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv = custom_arxiv._data\n",
    "arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PygNodePropPredDataset()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arx = PygNodePropPredDataset(name=\"ogbn-arxiv\", root=data_root)\n",
    "arx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': tensor([     0,      1,      2,  ..., 169145, 169148, 169251]),\n",
       " 'valid': tensor([   349,    357,    366,  ..., 169185, 169261, 169296]),\n",
       " 'test': tensor([   346,    398,    451,  ..., 169340, 169341, 169342])}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arx.get_idx_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_nodes <class 'int'>\n",
      "edge_index <class 'torch.Tensor'>\n",
      "x <class 'torch.Tensor'>\n",
      "node_year <class 'torch.Tensor'>\n",
      "y <class 'torch.Tensor'>\n",
      "x_text_feat <class 'torch.Tensor'>\n",
      "label_text_feat <class 'torch.Tensor'>\n",
      "edge_text_feat <class 'torch.Tensor'>\n",
      "prompt_text_feat <class 'torch.Tensor'>\n",
      "prompt_edge_feat <class 'torch.Tensor'>\n",
      "train_mask <class 'torch.Tensor'>\n",
      "val_mask <class 'torch.Tensor'>\n",
      "test_mask <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "for k, v in arxiv:\n",
    "    print(k, type(v))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_pyg_dgl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
