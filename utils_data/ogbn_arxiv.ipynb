{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ogb.nodeproppred import PygNodePropPredDataset\n",
    "import os\n",
    "import os.path as osp\n",
    "import sys\n",
    "import torch_geometric\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch_geometric.data import InMemoryDataset, download_url\n",
    "import random\n",
    "\n",
    "sys.path.append(osp.abspath(\"..\"))\n",
    "print(sys.path)\n",
    "\n",
    "from utils.encoder import SentenceEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the ogbn-arxiv dataset\n",
    "data_root = \"../data\"\n",
    "arxiv = PygNodePropPredDataset(name='ogbn-arxiv', root=data_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(num_nodes=169343, edge_index=[2, 1166243], x=[169343, 128], node_year=[169343, 1], y=[169343, 1])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = arxiv[0]\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_taxonomy(data_root):\n",
    "    # read categories and description file\n",
    "    f = open(osp.join(data_root, \"ogbn_arxiv\", \"arxiv_CS_categories.txt\"), \"r\").readlines()\n",
    "\n",
    "    state = 0\n",
    "    result = {\"id\": [], \"name\": [], \"description\": []}\n",
    "\n",
    "    for line in f:\n",
    "        if state == 0:\n",
    "            assert line.strip().startswith(\"cs.\")\n",
    "            category = (\"arxiv \"\n",
    "                + \" \".join(line.strip().split(\" \")[0].split(\".\")).lower())\n",
    "            # e.g. cs lo\n",
    "\n",
    "            name = line.strip()[7:-1]  # e. g. Logic in CS\n",
    "            result[\"id\"].append(category)\n",
    "            result[\"name\"].append(name)\n",
    "            state = 1\n",
    "            continue\n",
    "\n",
    "        elif state == 1:\n",
    "            description = line.strip()\n",
    "            result[\"description\"].append(description)\n",
    "            state = 2\n",
    "            continue\n",
    "\n",
    "        elif state == 2:\n",
    "            state = 0\n",
    "            continue\n",
    "\n",
    "    arxiv_cs_taxonomy = pd.DataFrame(result)\n",
    "    return arxiv_cs_taxonomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_feature(data_root):\n",
    "    nodeidx2paperid = pd.read_csv(osp.join(data_root, \"ogbn_arxiv/mapping/nodeidx2paperid.csv.gz\"), index_col=\"node idx\")\n",
    "\n",
    "    # Load the title and abstract of each paper\n",
    "    titleabs_url = \"https://snap.stanford.edu/ogb/data/misc/ogbn_arxiv/titleabs.tsv\"\n",
    "    titleabs_path = osp.join(data_root, \"ogbn_arxiv\", \"titleabs.tsv\")\n",
    "\n",
    "    if (osp.exists(titleabs_path)):\n",
    "        titleabs = pd.read_csv(titleabs_path, sep=\"\\t\", names=[\"paper id\", \"title\", \"abstract\"], index_col=\"paper id\")\n",
    "    else:\n",
    "        titleabs = pd.read_csv(titleabs_url, sep=\"\\t\", names=[\"paper id\", \"title\", \"abstract\"], index_col=\"paper id\")\n",
    "\n",
    "    titleabs = nodeidx2paperid.join(titleabs, on=\"paper id\")\n",
    "\n",
    "\n",
    "    # Prompt for the feature of nodes (can be changed accordingly)\n",
    "    node_feature_prompt = (\"Feature Node.\\n\"\n",
    "                        + \"Paper Title and Abstract : \"\n",
    "                        + titleabs[\"title\"]\n",
    "                        + \" + \"\n",
    "                        + titleabs[\"abstract\"])\n",
    "\n",
    "    node_feature_prompt_list = node_feature_prompt.values\n",
    "    return node_feature_prompt_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_feature(data_root):\n",
    "    arxiv_cs_taxonomy = get_taxonomy(data_root)\n",
    "\n",
    "    mapping_file = osp.join(data_root, \"ogbn_arxiv\", \"mapping\", \"labelidx2arxivcategeory.csv.gz\")\n",
    "    labelidx2arxivcategory = pd.read_csv(mapping_file)\n",
    "\n",
    "    arxiv_categ_vals = pd.merge(labelidx2arxivcategory, arxiv_cs_taxonomy, left_on=\"arxiv category\", right_on=\"id\")\n",
    "\n",
    "\n",
    "    # Prompt for the label nodes (can be changed accordingly)\n",
    "    label_node_prompt = (\"Prompt Node.\\n\"\n",
    "                        + \"Literature Category and Description: \"\n",
    "                        + arxiv_categ_vals[\"name\"]\n",
    "                        + \" + \"\n",
    "                        + arxiv_categ_vals[\"description\"])\n",
    "\n",
    "    label_node_prompt_list = label_node_prompt.values\n",
    "    return label_node_prompt_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArxivPyGDataset(InMemoryDataset):\n",
    "    def __init__(self, dataRoot=\"../data\", custom_dataRoot=\"../custom_data\", sentence_encoder=None, transform=None, pre_transform=None, pre_filter=None):\n",
    "        self.data_root = dataRoot\n",
    "        self.custom_data_root = custom_dataRoot\n",
    "        self.sentence_encoder = sentence_encoder\n",
    "        self.custom_data_dir = osp.join(self.custom_data_root, f\"ogbn_arxiv_{self.sentence_encoder.name}\")\n",
    "\n",
    "        if not osp.exists(self.custom_data_dir):\n",
    "            os.makedirs(self.custom_data_dir)\n",
    "\n",
    "        super().__init__(self.custom_data_dir, transform, pre_transform, pre_filter)\n",
    "\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return []\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return [\"data.pt\", \"texts.pkl\"]\n",
    "\n",
    "    def text_to_embed(self, texts):\n",
    "        if self.sentence_encoder is None:\n",
    "            raise NotImplementedError(\"Sentence Encoder is not passed\")\n",
    "        if texts is None:\n",
    "            return None\n",
    "        else:\n",
    "            return self.sentence_encoder.encode(texts) # returns to self.device\n",
    "\n",
    "    def encode_texts(self, texts):\n",
    "        if isinstance(texts[0], str):\n",
    "            return self.text_to_embed(texts)\n",
    "        return [self.text_to_embed(t) for t in texts]\n",
    "\n",
    "    def generate_custom_data(self):\n",
    "        node_texts = get_node_feature(self.data_root).tolist()\n",
    "        label_texts = get_label_feature(self.data_root).tolist()\n",
    "\n",
    "        # Prompt for prompt node/edge and edge texts (can be changed accordingly)\n",
    "        edge_texts = [\"Feature Edge.\\n Citation\"]\n",
    "        prompt_texts = [\"Prompt Node.\\n Node Classification of Literature Category\"]\n",
    "        prompt_edge_texts = [\"Prompt Edge.\"]\n",
    "\n",
    "        return [node_texts, label_texts, edge_texts, prompt_texts, prompt_edge_texts]\n",
    "\n",
    "    def process(self):\n",
    "        arxiv_data = PygNodePropPredDataset(name=\"ogbn-arxiv\", root=self.data_root)\n",
    "        arxiv_data_list = arxiv_data._data\n",
    "        arxiv_data_list.y = arxiv_data_list.y.squeeze()  # to flatten the y tensor\n",
    "\n",
    "        texts = self.generate_custom_data()\n",
    "        texts_embed = self.encode_texts(texts)\n",
    "\n",
    "        torch.save(texts, self.processed_paths[1])\n",
    "\n",
    "        arxiv_data_list.x_text_feat = texts_embed[0] # node text feature\n",
    "        arxiv_data_list.label_text_feat = texts_embed[1] # label text feature\n",
    "        arxiv_data_list.edge_text_feat = texts_embed[2] # edge text feature\n",
    "        arxiv_data_list.prompt_text_feat = texts_embed[3] # prompt node text feature\n",
    "        arxiv_data_list.prompt_edge_feat = texts_embed[4] # prompt edge text feature\n",
    "\n",
    "        # get dataset split\n",
    "        split_idx = arxiv_data.get_idx_split()\n",
    "        train_idx, valid_idx, test_idx = split_idx[\"train\"], split_idx[\"valid\"], split_idx[\"test\"]\n",
    "\n",
    "        arxiv_data_list.train_mask = train_idx\n",
    "        arxiv_data_list.val_mask = valid_idx\n",
    "        arxiv_data_list.test_mask = test_idx\n",
    "\n",
    "        data, slices = self.collate([arxiv_data_list]) # Pass the data_list as a list\n",
    "\n",
    "        torch.save((data, slices), self.processed_paths[0])\n",
    "        print(\"Arxiv is processed. Saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "LMencoder = SentenceEncoder(root=\"../lang_models\", name=\"ST\", device=1)\n",
    "custom_arxiv = ArxivPyGDataset(dataRoot=data_root, sentence_encoder=LMencoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "LMencoder2 = SentenceEncoder(root=\"../lang_models\", name=\"roberta\", device=1)\n",
    "custom_arxiv2 = ArxivPyGDataset(dataRoot=data_root, sentence_encoder=LMencoder2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(num_nodes=169343, edge_index=[2, 1166243], x=[169343, 128], node_year=[169343, 1], y=[169343], x_text_feat=[169343, 768], label_text_feat=[40, 768], edge_text_feat=[1, 768], prompt_text_feat=[1, 768], prompt_edge_feat=[1, 768], train_mask=[90941], val_mask=[29799], test_mask=[48603])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv = custom_arxiv._data\n",
    "arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PygNodePropPredDataset()"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arx = PygNodePropPredDataset(name=\"ogbn-arxiv\", root=data_root)\n",
    "arx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': tensor([     0,      1,      2,  ..., 169145, 169148, 169251]),\n",
       " 'valid': tensor([   349,    357,    366,  ..., 169185, 169261, 169296]),\n",
       " 'test': tensor([   346,    398,    451,  ..., 169340, 169341, 169342])}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arx.get_idx_split()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_pyg_dgl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
