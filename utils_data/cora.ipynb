{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/prateek/graphs-with-llms-experiments/utils_data', '/home/prateek/miniconda3/envs/torch_pyg_dgl/lib/python310.zip', '/home/prateek/miniconda3/envs/torch_pyg_dgl/lib/python3.10', '/home/prateek/miniconda3/envs/torch_pyg_dgl/lib/python3.10/lib-dynload', '', '/home/prateek/miniconda3/envs/torch_pyg_dgl/lib/python3.10/site-packages', '/home/prateek/graphs-with-llms-experiments']\n"
     ]
    }
   ],
   "source": [
    "from ogb.nodeproppred import PygNodePropPredDataset\n",
    "import os\n",
    "import os.path as osp\n",
    "import sys\n",
    "import torch_geometric\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch_geometric.data import InMemoryDataset, download_url\n",
    "\n",
    "sys.path.append(osp.abspath(\"..\"))\n",
    "print(sys.path)\n",
    "\n",
    "from utils.encoder import SentenceEncoder\n",
    "from utils_data.custom_pyg import CustomPygDataset\n",
    "from utils.dataloader import GetDataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the cora dataset\n",
    "data_root = \"../data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoraPyGDataset(InMemoryDataset, CustomPygDataset):\n",
    "    def __init__(self, dataRoot=\"../data\", custom_dataRoot=\"../custom_data\", sentence_encoder=None, transform=None, pre_transform=None, pre_filter=None):\n",
    "        self.data_root = dataRoot\n",
    "        self.custom_data_root = custom_dataRoot\n",
    "        self.sentence_encoder = sentence_encoder\n",
    "        self.custom_data_dir = osp.join(self.custom_data_root, f\"cora_{self.sentence_encoder.name}\")\n",
    "\n",
    "        if not osp.exists(self.custom_data_dir):\n",
    "            os.makedirs(self.custom_data_dir)\n",
    "\n",
    "        super().__init__(self.custom_data_dir, transform, pre_transform, pre_filter)\n",
    "\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return []\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return [\"data.pt\", \"texts.pkl\"]\n",
    "\n",
    "    def text_to_embed(self, texts):\n",
    "        if self.sentence_encoder is None:\n",
    "            raise NotImplementedError(\"Sentence Encoder is not passed\")\n",
    "        if texts is None:\n",
    "            return None\n",
    "        else:\n",
    "            return self.sentence_encoder.encode(texts)  # returns to self.device\n",
    "\n",
    "    def encode_texts(self, texts):\n",
    "        if isinstance(texts[0], str):\n",
    "            return self.text_to_embed(texts)\n",
    "        return [self.text_to_embed(t) for t in texts]\n",
    "\n",
    "    def generate_custom_data(self):\n",
    "        # Load the raw cora dataset\n",
    "        data_path = osp.join(self.data_root, \"cora\", \"cora.pt\")\n",
    "        raw_cora_data = torch.load(data_path)\n",
    "\n",
    "        texts = raw_cora_data.raw_text\n",
    "        label_names = raw_cora_data.label_names\n",
    "\n",
    "\n",
    "        # Label and label description\n",
    "        category_desc = pd.read_csv(osp.join(self.data_root, \"cora\", \"categories.csv\"), sep=\",\").values\n",
    "\n",
    "        # Sort the label desc by the order of label_names\n",
    "        ordered_desc = []\n",
    "        for i, label in enumerate(label_names):\n",
    "            true_ind = (label == category_desc[:, 0])\n",
    "            ordered_desc.append((label, category_desc[true_ind, 1][0]))\n",
    "\n",
    "        # Prompts for nodes/edges in original graph (can be changed accordingly)\n",
    "        node_texts = [\"Feature Node.\\n Paper Title and abstract: \" + t for t in texts]\n",
    "        edge_text = [\"Feature Edge.\\n Connected papers are cited together by other papers.\"]\n",
    "\n",
    "        # Node classification : Prompts for prompt node and label node (can be changed accordingly)\n",
    "        prompt_node_text = [\"Prompt Node.\\n Node Classification on the paper's category\"]\n",
    "        label_texts = [\"Prompt Node.\\n Literature Category and Description: \" + desc[0] + \" + \" + desc[1] for desc in ordered_desc]\n",
    "\n",
    "        # Link prediction : Prompts for prompt node and edge labels (can be changed accordingly)\n",
    "        prompt_node_edge_text = [\"Prompt Node.\\n Link Prediction on the papers that are cited together\"]\n",
    "        edge_label_text = [\"Prompt Node.\\n Two papers have co-citation\",\n",
    "                           \"Prompt Node.\\n Two papers do not have co-citation\"]\n",
    "\n",
    "        # Prompt for edge b/w prompt node and labels (can be changed accordingly)\n",
    "        prompt_edge_text = [\"Prompt Edge.\"]\n",
    "\n",
    "        return raw_cora_data, [node_texts, label_texts, edge_text, prompt_node_edge_text, prompt_node_text, prompt_edge_text, edge_label_text]\n",
    "\n",
    "    def process(self):\n",
    "        # raw cora dataset is not in any library, so we process and load it manually in self.generate_custom_data()\n",
    "        cora_data_list, texts = self.generate_custom_data()\n",
    "        texts_embed = self.encode_texts(texts)\n",
    "\n",
    "        torch.save(texts, self.processed_paths[1])\n",
    "\n",
    "        cora_data_list.x_text_feat = texts_embed[0]\n",
    "        cora_data_list.label_text_feat = texts_embed[1]\n",
    "        cora_data_list.edge_text_feat = texts_embed[2]\n",
    "        cora_data_list.prompt_text_edge_feat = texts_embed[3]\n",
    "        cora_data_list.prompt_text_feat = texts_embed[4]\n",
    "        cora_data_list.prompt_edge_feat = texts_embed[5]\n",
    "        cora_data_list.edge_label_feat = texts_embed[6]\n",
    "\n",
    "        cora_data_list.train_mask = cora_data_list.train_masks[0]\n",
    "        cora_data_list.val_mask = cora_data_list.val_masks[0]\n",
    "        cora_data_list.test_mask = cora_data_list.test_masks[0]\n",
    "\n",
    "        cora_data_list.train_masks = None\n",
    "        cora_data_list.val_masks = None\n",
    "        cora_data_list.test_masks = None\n",
    "\n",
    "        # Pass the data_list as a list\n",
    "        data, slices = self.collate([cora_data_list])\n",
    "\n",
    "        torch.save((data, slices), self.processed_paths[0])\n",
    "        print(\"Cora is processed. Saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LMencoder = SentenceEncoder(\"ST\", root=\"../lang_models\", device=2)\n",
    "custom_cora = CoraPyGDataset(dataRoot=data_root, sentence_encoder=LMencoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "LMencoder2 = SentenceEncoder(\"roberta\", root=\"../lang_models\", device=2)\n",
    "custom_cora2 = CoraPyGDataset(dataRoot=data_root, sentence_encoder=LMencoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cora = custom_cora._data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(raw_text=[1], y=[2708], label_names=[1], edge_index=[2, 10858], x=[2708, 384], category_names=[1], x_text_feat=[2708, 768], label_text_feat=[7, 768], edge_text_feat=[1, 768], prompt_text_edge_feat=[1, 768], prompt_text_feat=[1, 768], prompt_edge_feat=[1, 768], edge_label_feat=[2, 768], train_mask=[2708], val_mask=[2708], test_mask=[2708], batch=[2708], ptr=[2])\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "dl = DataLoader(custom_cora, batch_size=12)\n",
    "\n",
    "for batch in dl:\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(custom_cora, CustomPygDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CoraPyGDataset()\n"
     ]
    }
   ],
   "source": [
    "def f(var : CustomPygDataset):\n",
    "    print(var)\n",
    "\n",
    "f(custom_cora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2,  ..., 1, 1, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = cora.y.squeeze()\n",
    "print(label)\n",
    "label.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_cora.num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exp_name': 'Date -> 2024-01-30. Experiment_ST_evaluation-mode',\n",
       " 'dataRoot': '../data',\n",
       " 'custom_dataRoot': '../custom_data',\n",
       " 'dataset': 'cora',\n",
       " 'model_type': 'MLP',\n",
       " 'sentence_encoder': 'ST',\n",
       " 'encoder_path': '../lang_models',\n",
       " 'state_dict_path': './state_dicts',\n",
       " 'lr': 0.001,\n",
       " 'epochs': 200,\n",
       " 'batch_count': 5,\n",
       " 'batch_size': 5,\n",
       " 'weight_decay': 0.001,\n",
       " 'dropout': 0.3,\n",
       " 'seed': None,\n",
       " 'num_workers': 10,\n",
       " 'device': 'cuda:0',\n",
       " 'eval_only': False,\n",
       " 'n_way': 3,\n",
       " 'n_shot': 3,\n",
       " 'n_query': 2}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import yaml\n",
    "from datetime import date\n",
    "\n",
    "with open(\"../config.yaml\", \"r\") as f:\n",
    "    args = yaml.safe_load(f)\n",
    "\n",
    "args[\"device\"] = 'cpu' if args[\"device\"] == 123 else f\"cuda:{args['device']}\"\n",
    "args[\"exp_name\"] = f\"Date -> {date.today()}. Experiment_{args['sentence_encoder']}_{args['exp_name']}\"\n",
    "\n",
    "args[\"encoder_path\"] = '../lang_models'\n",
    "args[\"dataRoot\"] = '../data'\n",
    "args[\"custom_dataRoot\"] = '../custom_data'\n",
    "args[\"dataset\"] = \"cora\"\n",
    "args[\"batch_count\"] = 5\n",
    "\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<utils.dataloader.GetDataloader at 0x7f0c702287f0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl = GetDataloader(**args)\n",
    "dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl.get_num_classes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{6: [797, 959, 1610, 2217, 1554],\n",
       "   3: [2246, 2555, 2116, 299, 58],\n",
       "   1: [579, 445, 1766, 2419, 660]},\n",
       "  {6: [2277, 2191, 1087, 221, 160],\n",
       "   0: [518, 1800, 2414, 2622, 886],\n",
       "   1: [1220, 495, 2397, 1195, 445]},\n",
       "  {3: [993, 2116, 841, 1776, 58],\n",
       "   0: [2458, 9, 1736, 1878, 2622],\n",
       "   1: [1017, 1195, 445, 250, 495]},\n",
       "  {2: [1602, 1263, 1499, 1751, 658],\n",
       "   3: [1004, 2555, 739, 856, 841],\n",
       "   0: [518, 2325, 1583, 886, 1800]},\n",
       "  {5: [1895, 2205, 2594, 789, 2482],\n",
       "   3: [2137, 534, 993, 739, 841],\n",
       "   6: [221, 2347, 1971, 1554, 797]}],\n",
       " [{6: [2013, 1087, 2299, 1554, 1970],\n",
       "   2: [954, 2150, 547, 1602, 2466],\n",
       "   4: [2392, 516, 507, 1246, 1950]},\n",
       "  {3: [856, 534, 1302, 2348, 2263],\n",
       "   1: [250, 2701, 1220, 1195, 72],\n",
       "   0: [1186, 518, 1875, 1736, 2414]},\n",
       "  {6: [2191, 2337, 2217, 1234, 2299],\n",
       "   1: [1017, 380, 2397, 445, 1],\n",
       "   2: [845, 576, 547, 954, 1263]},\n",
       "  {2: [954, 231, 658, 2242, 174],\n",
       "   5: [1418, 1519, 2482, 1562, 2181],\n",
       "   0: [886, 2590, 412, 322, 1878]},\n",
       "  {4: [1315, 1642, 2519, 1950, 2067],\n",
       "   1: [495, 2397, 2323, 579, 2701],\n",
       "   0: [144, 2414, 1875, 2590, 2325]}],\n",
       " [{2: [1499, 1602, 576, 954, 5],\n",
       "   0: [1736, 322, 412, 1800, 1373],\n",
       "   4: [1315, 2519, 1825, 2559, 516]},\n",
       "  {2: [1499, 2466, 2467, 845, 1696],\n",
       "   4: [1825, 152, 40, 1440, 2067],\n",
       "   3: [299, 534, 2555, 2112, 993]},\n",
       "  {5: [1562, 2482, 2282, 348, 2205],\n",
       "   1: [1662, 495, 2419, 1129, 660],\n",
       "   0: [1583, 677, 1373, 9, 886]},\n",
       "  {3: [1302, 2033, 739, 299, 2137],\n",
       "   0: [2622, 1875, 2414, 1583, 322],\n",
       "   6: [1554, 1970, 2217, 2191, 1971]},\n",
       "  {2: [231, 1602, 658, 1499, 5],\n",
       "   5: [2282, 1741, 2183, 334, 1519],\n",
       "   1: [250, 1129, 495, 380, 1]}],\n",
       " [{1: [445, 2323, 1, 2701, 1195],\n",
       "   3: [2348, 856, 1302, 534, 2137],\n",
       "   0: [1583, 886, 322, 1029, 1875]},\n",
       "  {6: [160, 1970, 728, 1554, 1234],\n",
       "   5: [2181, 1041, 789, 1221, 1645],\n",
       "   2: [658, 2466, 1343, 182, 1696]},\n",
       "  {2: [576, 182, 2466, 5, 231],\n",
       "   3: [1776, 2263, 2348, 2112, 739],\n",
       "   4: [2392, 688, 1642, 1440, 1315]},\n",
       "  {5: [2181, 1418, 1741, 2594, 1645],\n",
       "   0: [2458, 1875, 9, 2325, 518],\n",
       "   4: [2363, 40, 1440, 516, 1950]},\n",
       "  {0: [9, 886, 677, 1029, 2622],\n",
       "   3: [2348, 1004, 739, 505, 2116],\n",
       "   5: [2181, 2205, 358, 1041, 2183]}],\n",
       " [{2: [2150, 1499, 2467, 1343, 1751],\n",
       "   0: [1878, 1029, 1583, 518, 2458],\n",
       "   4: [330, 507, 2067, 2417, 1177]},\n",
       "  {1: [72, 2701, 380, 1129, 579],\n",
       "   0: [1800, 677, 1029, 2458, 2414],\n",
       "   2: [174, 1696, 547, 576, 182]},\n",
       "  {3: [299, 993, 58, 505, 2263],\n",
       "   1: [1195, 250, 1220, 2701, 1503],\n",
       "   5: [1562, 348, 1741, 1895, 1041]},\n",
       "  {3: [2033, 856, 2030, 2112, 2263],\n",
       "   6: [2217, 221, 160, 2337, 2277],\n",
       "   1: [495, 1511, 2701, 72, 445]},\n",
       "  {3: [2246, 993, 58, 1004, 1776],\n",
       "   4: [2519, 2343, 2417, 1177, 152],\n",
       "   1: [495, 1195, 1511, 72, 1766]}]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(dl.trn_smplr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['raw_text', 'y', 'label_names', 'edge_index', 'x', 'category_names', 'x_text_feat', 'label_text_feat', 'edge_text_feat', 'prompt_text_edge_feat', 'prompt_text_feat', 'prompt_edge_feat', 'edge_label_feat', 'train_mask', 'val_mask', 'test_mask']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['raw_text',\n",
       " 'y',\n",
       " 'x',\n",
       " 'category_names',\n",
       " 'x_text_feat',\n",
       " 'train_mask',\n",
       " 'val_mask',\n",
       " 'test_mask']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = []\n",
    "for key, value in cora:\n",
    "    a.append(key)\n",
    "print(a)\n",
    "\n",
    "node_attrs = [key for key, value in cora if cora.is_node_attr(key)]\n",
    "node_attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(raw_text=[2708], y=[2708], label_names=[7], edge_index=[2, 10858], x=[2708, 384], category_names=[2708], x_text_feat=[2708, 768], label_text_feat=[7, 768], edge_text_feat=[1, 768], prompt_text_edge_feat=[1, 768], prompt_text_feat=[1, 768], prompt_edge_feat=[1, 768], edge_label_feat=[2, 768], train_mask=[2708], val_mask=[2708], test_mask=[2708])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cora.to(\"cpu\")\n",
    "cora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw_text <class 'list'>\n",
      "y <class 'torch.Tensor'>\n",
      "label_names <class 'list'>\n",
      "edge_index <class 'torch.Tensor'>\n",
      "x <class 'torch.Tensor'>\n",
      "category_names <class 'list'>\n",
      "x_text_feat <class 'torch.Tensor'>\n",
      "label_text_feat <class 'torch.Tensor'>\n",
      "edge_text_feat <class 'torch.Tensor'>\n",
      "prompt_text_edge_feat <class 'torch.Tensor'>\n",
      "prompt_text_feat <class 'torch.Tensor'>\n",
      "prompt_edge_feat <class 'torch.Tensor'>\n",
      "edge_label_feat <class 'torch.Tensor'>\n",
      "train_mask <class 'torch.Tensor'>\n",
      "val_mask <class 'torch.Tensor'>\n",
      "test_mask <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "for k, v in cora:\n",
    "    print(k, type(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(raw_text=[1000], y=[1000], label_names=[7], edge_index=[2, 556], x=[1000, 384], category_names=[1000], x_text_feat=[1000, 768], label_text_feat=[7, 768], edge_text_feat=[1, 768], prompt_text_edge_feat=[1, 768], prompt_text_feat=[1, 768], prompt_edge_feat=[1, 768], edge_label_feat=[2, 768], train_mask=[1000], val_mask=[1000], test_mask=[1000])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = torch.Tensor(range(1000)).type(torch.int)\n",
    "\n",
    "sg = cora.subgraph(idx)\n",
    "sg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "292"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = sg.edge_index[0]\n",
    "j = sg.edge_index[1]\n",
    "\n",
    "i.unique().shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(y=[2708], label_names=[7], edge_index=[2, 10858], x=[2708, 384], x_text_feat=[2708, 768], label_text_feat=[7, 768], edge_text_feat=[1, 768], prompt_text_edge_feat=[1, 768], prompt_text_feat=[1, 768], prompt_edge_feat=[1, 768], edge_label_feat=[2, 768], train_mask=[2708], val_mask=[2708], test_mask=[2708])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cora.raw_text = None\n",
    "cora.category_names = None\n",
    "cora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import NeighborLoader, NodeLoader\n",
    "\n",
    "loader = None\n",
    "loader = NeighborLoader(data=cora,\n",
    "                        num_neighbors=[-1],\n",
    "                        input_nodes=torch.LongTensor([0, 1, 2]),\n",
    "                        subgraph_type=)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(y=[6], label_names=[7], edge_index=[2, 5], x=[6, 384], x_text_feat=[6, 768], label_text_feat=[7, 768], edge_text_feat=[1, 768], prompt_text_edge_feat=[1, 768], prompt_text_feat=[1, 768], prompt_edge_feat=[1, 768], edge_label_feat=[2, 768], train_mask=[6], val_mask=[6], test_mask=[6], n_id=[6], e_id=[5], num_sampled_nodes=[2], num_sampled_edges=[1], input_id=[1], batch_size=1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sg = next(iter(loader))\n",
    "sg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y tensor([0, 0, 4, 0, 0, 0])\n",
      "label_names ['Rule_Learning', 'Neural_Networks', 'Case_Based', 'Genetic_Algorithms', 'Theory', 'Reinforcement_Learning', 'Probabilistic_Methods']\n",
      "edge_index tensor([[1, 2, 3, 4, 5],\n",
      "        [0, 0, 0, 0, 0]])\n",
      "x tensor([[ 0.0087,  0.0294,  0.0358,  ...,  0.0481,  0.0118,  0.0109],\n",
      "        [-0.0271,  0.0552,  0.0554,  ...,  0.0173,  0.0651, -0.0354],\n",
      "        [-0.0349, -0.0170,  0.0448,  ..., -0.0737, -0.0317,  0.0582],\n",
      "        [ 0.0180, -0.0094,  0.0315,  ...,  0.0871,  0.0109,  0.0333],\n",
      "        [-0.0007,  0.0180, -0.0089,  ..., -0.0348,  0.0422,  0.0276],\n",
      "        [ 0.0344,  0.0096, -0.0080,  ...,  0.0427,  0.0105,  0.0244]])\n",
      "x_text_feat tensor([[ 0.0178, -0.0104, -0.0473,  ..., -0.0147, -0.0497,  0.0113],\n",
      "        [-0.0117,  0.0400, -0.0301,  ..., -0.0606, -0.0444, -0.0081],\n",
      "        [-0.0722, -0.0166, -0.0333,  ...,  0.0085, -0.0148,  0.0588],\n",
      "        [ 0.0063,  0.0369, -0.0176,  ...,  0.0040, -0.0289, -0.0065],\n",
      "        [ 0.0278, -0.0036, -0.0721,  ...,  0.0401, -0.0131,  0.0387],\n",
      "        [ 0.0002,  0.0156, -0.0226,  ..., -0.0330, -0.0485, -0.0038]])\n",
      "label_text_feat tensor([[-0.0052,  0.0024, -0.0413,  ..., -0.0615, -0.0563, -0.0037],\n",
      "        [ 0.0124, -0.0391,  0.0031,  ..., -0.0439, -0.0431,  0.0038],\n",
      "        [ 0.0232,  0.0143, -0.0349,  ..., -0.0522, -0.0292, -0.0393],\n",
      "        ...,\n",
      "        [ 0.0139,  0.0249, -0.0124,  ..., -0.0668, -0.0040, -0.0140],\n",
      "        [ 0.0216, -0.0068, -0.0158,  ..., -0.0703, -0.0779, -0.0292],\n",
      "        [-0.0144,  0.0349, -0.0328,  ..., -0.1053, -0.0512, -0.0174]])\n",
      "edge_text_feat tensor([[-0.0147, -0.0066, -0.0532,  0.0301, -0.0217, -0.0482, -0.0820, -0.0261,\n",
      "         -0.0100,  0.0199, -0.0021, -0.0078, -0.0228,  0.0358, -0.0109,  0.0694,\n",
      "          0.0012, -0.0300,  0.0567, -0.0245,  0.0229,  0.0380,  0.0037,  0.0242,\n",
      "          0.0165, -0.0418, -0.0063, -0.0137,  0.0135,  0.0534, -0.0417,  0.0635,\n",
      "          0.0358,  0.0220,  0.0284, -0.0255, -0.0481, -0.0380, -0.0223,  0.0527,\n",
      "          0.0125,  0.0796, -0.0440, -0.0030,  0.0466,  0.0136, -0.0626,  0.0613,\n",
      "          0.0038,  0.0548, -0.0571, -0.0366, -0.0029,  0.0042, -0.0025,  0.0125,\n",
      "          0.0591,  0.0120,  0.0052, -0.0513,  0.0120,  0.0188,  0.0714,  0.0141,\n",
      "          0.0019,  0.0251,  0.0185, -0.0042, -0.0015,  0.0118, -0.0373,  0.0214,\n",
      "         -0.0255,  0.0034,  0.0393,  0.0275,  0.0226,  0.0027,  0.0296, -0.0377,\n",
      "          0.0226,  0.0098, -0.0933,  0.0278, -0.0201,  0.0619, -0.0441, -0.0435,\n",
      "         -0.1010, -0.0520, -0.0561, -0.0151,  0.0158,  0.0341, -0.0087, -0.0126,\n",
      "          0.0106, -0.0260,  0.0212,  0.0231, -0.0164, -0.0669, -0.0085,  0.0523,\n",
      "         -0.0305, -0.0052, -0.0106, -0.0296,  0.0213, -0.0299, -0.0389, -0.0063,\n",
      "         -0.0747,  0.0225,  0.0141,  0.0849,  0.0019, -0.0441, -0.0322, -0.0274,\n",
      "         -0.0458,  0.0316, -0.0781,  0.0527,  0.0087, -0.0056,  0.0240, -0.0016,\n",
      "         -0.0325,  0.0280, -0.0667, -0.0206, -0.0420, -0.0025, -0.0047, -0.0240,\n",
      "          0.0539,  0.0665, -0.0057,  0.0185,  0.0401,  0.0052, -0.0224, -0.0698,\n",
      "          0.0364, -0.0125, -0.0008,  0.0377,  0.0345, -0.0302, -0.0167,  0.0382,\n",
      "         -0.0149,  0.0385, -0.0092,  0.0509,  0.0277,  0.0174, -0.0104, -0.0200,\n",
      "          0.0227, -0.0224,  0.0051,  0.0036, -0.0396, -0.0171,  0.0125, -0.0038,\n",
      "         -0.0419, -0.0009, -0.0025, -0.0306, -0.0065, -0.0030,  0.0093,  0.0438,\n",
      "         -0.0096, -0.0127,  0.0328,  0.0029, -0.0316,  0.0615, -0.0113, -0.0593,\n",
      "         -0.0461,  0.0330,  0.0499, -0.0322, -0.0231,  0.0374,  0.0162, -0.0213,\n",
      "          0.0008, -0.0173,  0.0337, -0.0101,  0.0482, -0.0297, -0.0246, -0.0231,\n",
      "          0.0088, -0.0057,  0.0303, -0.0292, -0.0656,  0.0159,  0.0347, -0.0115,\n",
      "         -0.0081, -0.0860,  0.0199,  0.0068, -0.0215, -0.0207,  0.0451,  0.0122,\n",
      "          0.0375,  0.0370,  0.0066, -0.0364,  0.0849,  0.0197,  0.0562,  0.0242,\n",
      "          0.0337,  0.0524,  0.0184,  0.0747, -0.0091,  0.0441, -0.0302,  0.0435,\n",
      "          0.0362, -0.1040,  0.0264, -0.0099, -0.0027,  0.0002,  0.0470, -0.0188,\n",
      "         -0.0227,  0.0465, -0.0093,  0.0007,  0.0621, -0.0409,  0.0192,  0.0153,\n",
      "          0.0169,  0.0159,  0.0174,  0.0229, -0.0389, -0.0349, -0.0035,  0.0452,\n",
      "         -0.0088,  0.0123,  0.0255, -0.0078,  0.0620, -0.0689, -0.0398, -0.0166,\n",
      "         -0.0194, -0.0104,  0.0226, -0.0330,  0.0449, -0.0591,  0.0488,  0.0061,\n",
      "         -0.0022, -0.0075, -0.0371, -0.0450,  0.0519,  0.0296, -0.0456, -0.0249,\n",
      "         -0.0090,  0.0167,  0.0418, -0.0668, -0.0091,  0.0206, -0.0014,  0.0163,\n",
      "          0.0293,  0.0504, -0.0021,  0.0203, -0.0595,  0.0300,  0.0508,  0.0491,\n",
      "         -0.0047,  0.0132, -0.0730, -0.0083,  0.0214, -0.0570,  0.0316, -0.0300,\n",
      "          0.0075,  0.0325, -0.0583,  0.0133, -0.0344, -0.0219,  0.0089,  0.0125,\n",
      "         -0.0112,  0.0086,  0.0142,  0.0179, -0.0018, -0.0745,  0.0394, -0.0572,\n",
      "         -0.0092, -0.0088,  0.0201,  0.0434, -0.0022, -0.0707,  0.0322,  0.0167,\n",
      "         -0.0291, -0.0314, -0.0613, -0.0233, -0.0214, -0.0269, -0.0565,  0.0269,\n",
      "         -0.0025, -0.0136,  0.0340, -0.0540, -0.0361, -0.0002,  0.0147, -0.0367,\n",
      "          0.0084,  0.0536, -0.0099,  0.0554,  0.0368,  0.0606,  0.0148, -0.0534,\n",
      "          0.0739, -0.0149, -0.0158, -0.0127, -0.0300,  0.0083,  0.0238,  0.0127,\n",
      "          0.0461,  0.0070, -0.0150, -0.0442, -0.0158, -0.0698, -0.0616, -0.0231,\n",
      "         -0.0101,  0.0326,  0.0092, -0.0237, -0.0648, -0.0185, -0.0158, -0.0408,\n",
      "          0.0079, -0.0504, -0.0506,  0.1341, -0.0170, -0.0231, -0.0826,  0.0041,\n",
      "          0.0707, -0.0350, -0.0256,  0.0050,  0.0085,  0.0212, -0.0127,  0.0316,\n",
      "          0.0013,  0.0325,  0.0066,  0.0014, -0.0240,  0.0231, -0.0170,  0.0351,\n",
      "          0.0171,  0.0537, -0.0521, -0.0048,  0.0110,  0.0447, -0.0142,  0.0022,\n",
      "          0.0434,  0.0036,  0.0019, -0.0420, -0.0104,  0.0568, -0.0177, -0.0598,\n",
      "         -0.0106,  0.0075, -0.0500, -0.0102,  0.0053,  0.0362,  0.0562,  0.0270,\n",
      "         -0.0013, -0.0010, -0.0165, -0.0177, -0.0048,  0.0607,  0.0395, -0.0410,\n",
      "          0.0207, -0.0371, -0.0456, -0.0038,  0.0134,  0.0434, -0.0062,  0.0209,\n",
      "         -0.0119,  0.0040, -0.0502, -0.0165, -0.0360,  0.0055, -0.0108, -0.0014,\n",
      "          0.0225, -0.0021, -0.0005, -0.0285,  0.1086,  0.0039, -0.0454,  0.0210,\n",
      "         -0.0112,  0.0748, -0.0181, -0.0216, -0.0216,  0.0196,  0.0292,  0.0229,\n",
      "          0.0051,  0.0073, -0.0036,  0.0426,  0.0053,  0.0513,  0.0317,  0.0107,\n",
      "         -0.0495, -0.0223, -0.0440, -0.0006,  0.0090, -0.0653, -0.0308, -0.0160,\n",
      "          0.0672, -0.0186, -0.0148,  0.0680, -0.0069, -0.0501,  0.0617,  0.0183,\n",
      "          0.0373,  0.0482, -0.0268,  0.0772, -0.0061, -0.0234,  0.0389, -0.0148,\n",
      "          0.0129,  0.0070,  0.0714,  0.0091,  0.0430,  0.0030,  0.0206, -0.0167,\n",
      "          0.0333, -0.0678, -0.0184, -0.0743, -0.0324,  0.0289,  0.0192,  0.0375,\n",
      "         -0.0538, -0.0140, -0.0505, -0.0449, -0.0687, -0.0490, -0.0125,  0.0040,\n",
      "         -0.0630,  0.0066,  0.0859,  0.0280,  0.0538,  0.0065, -0.0372,  0.0545,\n",
      "          0.0378, -0.0461,  0.0128, -0.0504,  0.0261,  0.0374,  0.0039,  0.0496,\n",
      "         -0.0188, -0.0543, -0.0236, -0.0203, -0.0130, -0.0337, -0.0154,  0.0420,\n",
      "         -0.0532, -0.0064,  0.0425, -0.0106, -0.0318, -0.0168, -0.0501,  0.0067,\n",
      "          0.0032, -0.0025,  0.0065, -0.0046,  0.0028, -0.0164, -0.0124, -0.0111,\n",
      "          0.0135,  0.0067,  0.0397,  0.0606, -0.0373, -0.0147,  0.0041,  0.0043,\n",
      "         -0.0205, -0.0598,  0.0533, -0.0112,  0.0004, -0.0359, -0.0011, -0.0008,\n",
      "          0.0008,  0.0228, -0.0077,  0.0241,  0.0275, -0.0113, -0.0629,  0.0688,\n",
      "         -0.0307,  0.0599,  0.0085, -0.0038,  0.0289, -0.0609, -0.0297,  0.0105,\n",
      "         -0.0055,  0.0319,  0.0198,  0.0321,  0.0009, -0.0785,  0.0157, -0.0235,\n",
      "         -0.0658,  0.0090,  0.0271, -0.0642,  0.0198,  0.0003, -0.0382,  0.0781,\n",
      "          0.0017, -0.0435, -0.0240, -0.0575,  0.0487,  0.0444,  0.0101, -0.0238,\n",
      "          0.0035, -0.0156,  0.0714,  0.0112,  0.0110,  0.0138, -0.0032,  0.0338,\n",
      "          0.0021,  0.0273,  0.0506, -0.0643, -0.0518, -0.0157,  0.0648, -0.0232,\n",
      "          0.0345, -0.0059,  0.0587, -0.0023, -0.0529,  0.1054,  0.0482,  0.0387,\n",
      "          0.0281,  0.0487,  0.0197,  0.0302,  0.0303,  0.0010, -0.0035, -0.0025,\n",
      "         -0.0194, -0.0063, -0.0045,  0.0286, -0.0074,  0.0142,  0.0236,  0.0294,\n",
      "         -0.0063,  0.0453,  0.0241,  0.0071, -0.0014,  0.0611,  0.0033, -0.0157,\n",
      "          0.0338,  0.0316,  0.0193, -0.0206, -0.0312,  0.0641, -0.0052, -0.0053,\n",
      "         -0.0306, -0.0424,  0.0202,  0.0236, -0.0166,  0.0181,  0.0044, -0.0120,\n",
      "         -0.0651,  0.0244, -0.0185,  0.0176,  0.0268, -0.0637,  0.0379,  0.0310,\n",
      "          0.0053,  0.0462, -0.0520,  0.0247, -0.0444, -0.0555,  0.0071,  0.0695,\n",
      "          0.0399, -0.0674, -0.0381, -0.0063, -0.0013,  0.0270, -0.0142,  0.0420,\n",
      "         -0.0122,  0.0651,  0.0388, -0.0217,  0.0189, -0.0593, -0.0052,  0.0413,\n",
      "          0.0293,  0.0408, -0.0181, -0.0274,  0.0316,  0.0098, -0.0218, -0.0562,\n",
      "         -0.0478, -0.0035,  0.0366, -0.0268, -0.0176,  0.0089, -0.0021, -0.0812,\n",
      "          0.0688,  0.0287,  0.0717, -0.0599, -0.0014, -0.0405,  0.0565,  0.0652,\n",
      "          0.0260, -0.0358,  0.0547, -0.0527, -0.0570,  0.0190, -0.0087, -0.0660,\n",
      "         -0.0566, -0.0368,  0.0029, -0.0063, -0.0019,  0.0325, -0.0156, -0.0358,\n",
      "          0.0554,  0.0169,  0.0175,  0.0435,  0.0194,  0.0400,  0.0069,  0.0461,\n",
      "         -0.0470, -0.0131,  0.0170,  0.0184, -0.0058,  0.0040,  0.0217, -0.0570]])\n",
      "prompt_text_edge_feat tensor([[-1.3335e-02,  1.2672e-02,  8.9573e-04, -2.4598e-02, -1.0511e-02,\n",
      "         -6.0561e-02, -1.8646e-02, -7.1084e-03,  1.2146e-02, -2.0980e-02,\n",
      "         -1.7734e-02,  2.1645e-02, -3.7849e-02,  3.5755e-02,  2.0444e-02,\n",
      "          3.0474e-02, -3.6158e-02, -3.5071e-02,  1.4829e-02, -1.2173e-02,\n",
      "          6.0241e-03,  2.8635e-02,  9.8435e-03,  1.9259e-02,  5.4263e-02,\n",
      "         -8.4151e-04,  1.0006e-02, -2.8099e-02, -3.4299e-02,  7.1542e-04,\n",
      "         -5.2333e-02,  5.7011e-02,  1.5138e-02, -1.6199e-02, -2.6805e-02,\n",
      "          4.2219e-02,  3.9933e-03, -7.5779e-03,  2.2580e-02,  6.7279e-02,\n",
      "         -8.2448e-03,  4.4444e-02, -5.8606e-03,  6.4709e-02,  1.2761e-02,\n",
      "         -3.7561e-03, -3.9524e-02,  3.6785e-02,  2.0088e-02, -3.2027e-02,\n",
      "         -6.2909e-02, -3.3877e-02, -3.2821e-04,  1.0901e-02, -3.2370e-02,\n",
      "         -2.2289e-02,  2.9462e-02,  3.4995e-04,  1.2567e-03, -2.0942e-02,\n",
      "         -4.6462e-02,  8.4903e-03,  3.7903e-02,  5.9346e-02,  3.6110e-02,\n",
      "         -5.7784e-02,  4.7074e-02,  5.6215e-03, -5.8153e-02, -5.0689e-02,\n",
      "         -2.3460e-02,  4.0375e-02, -1.3250e-02, -7.4489e-03,  6.6279e-02,\n",
      "          3.9793e-02,  1.1567e-02,  2.8912e-02,  6.9058e-03, -5.6758e-02,\n",
      "          1.1123e-02, -5.6002e-02, -1.8333e-02, -3.3499e-02,  3.8968e-03,\n",
      "          1.2748e-02, -6.3607e-03, -2.8736e-02,  1.4413e-02, -6.2587e-02,\n",
      "         -5.3345e-02,  7.0895e-03, -5.0778e-02,  1.2680e-02, -2.7451e-02,\n",
      "          5.4535e-02, -8.0290e-03, -1.4363e-02,  1.4357e-02,  3.9457e-03,\n",
      "          6.1333e-02, -2.4427e-02,  2.1563e-02, -1.1847e-03, -4.8625e-02,\n",
      "          1.0852e-02, -2.2877e-03, -3.4335e-02,  6.5907e-02,  1.9567e-02,\n",
      "          2.4653e-02,  1.9095e-02,  2.5495e-02,  9.9545e-03, -1.5381e-02,\n",
      "          5.5682e-02, -2.1630e-02, -1.6332e-02, -2.9157e-02, -3.1116e-03,\n",
      "         -1.6643e-02,  6.0157e-02, -4.8060e-02,  2.8102e-02, -1.2881e-02,\n",
      "          3.2708e-02,  1.1007e-02, -2.1797e-02, -6.1962e-02, -1.0938e-02,\n",
      "          6.9563e-03,  5.3058e-03, -4.7702e-02, -1.6157e-02, -5.2604e-03,\n",
      "         -6.1625e-03, -1.7911e-02,  5.8146e-02,  7.5375e-02,  4.8208e-02,\n",
      "          6.8252e-03, -3.2451e-02,  1.8802e-02,  3.4895e-02,  2.9628e-02,\n",
      "         -1.3115e-03,  8.6707e-03,  1.4744e-02,  2.4699e-02,  2.0018e-02,\n",
      "         -7.6724e-03,  1.5912e-02, -6.3314e-02, -1.2640e-02, -6.4062e-03,\n",
      "          7.3781e-02,  3.8369e-02, -4.3632e-02,  7.1394e-03,  4.6414e-03,\n",
      "          5.2233e-02,  1.8496e-02, -4.8762e-03,  1.2926e-02, -2.9697e-02,\n",
      "         -2.5937e-02, -3.6481e-02, -1.9653e-02, -3.2687e-02, -6.9029e-03,\n",
      "          1.0996e-02, -8.1635e-04,  1.2145e-02, -4.3770e-02,  1.6464e-02,\n",
      "         -1.0581e-02, -2.6225e-02,  3.6733e-04,  1.0051e-02, -1.0133e-02,\n",
      "         -3.2907e-02,  8.0852e-02, -2.4917e-02, -5.8606e-02,  1.8615e-02,\n",
      "          1.6106e-02,  4.5299e-02, -1.4398e-02,  8.5588e-03,  7.2389e-02,\n",
      "          2.3506e-02,  3.0961e-02,  3.3072e-02,  5.8868e-04,  4.4747e-02,\n",
      "         -1.4441e-02,  4.0113e-02,  3.9935e-02, -3.6604e-02, -1.1574e-01,\n",
      "          5.1101e-02,  4.3315e-02,  7.5328e-02, -3.8490e-02, -1.3199e-02,\n",
      "          1.7231e-02, -3.4788e-03, -5.9194e-02, -1.0179e-02, -2.1146e-02,\n",
      "         -4.3691e-02,  4.9920e-02, -3.9695e-02,  1.6267e-02,  2.9494e-02,\n",
      "          7.2327e-03,  4.6895e-02, -2.6956e-02, -4.2970e-02,  6.3043e-03,\n",
      "          6.4480e-02,  4.2479e-02,  2.5360e-02,  2.9140e-02,  2.3833e-02,\n",
      "          4.2073e-02,  4.9342e-04,  4.4643e-02, -4.7565e-02,  4.6169e-02,\n",
      "          1.4820e-02,  4.3295e-02,  4.2425e-02, -1.2661e-02,  4.2572e-02,\n",
      "         -5.5756e-02,  2.4092e-03,  2.4044e-02, -1.1228e-02,  1.4564e-02,\n",
      "          3.2327e-02, -2.6365e-02, -1.1404e-01,  4.7127e-02,  2.2411e-02,\n",
      "         -5.0031e-02, -3.9888e-02,  4.0838e-03, -1.5090e-02,  8.1097e-03,\n",
      "          6.9695e-03, -3.4031e-02, -5.7577e-02, -3.6286e-02,  1.2303e-02,\n",
      "         -1.1208e-02, -4.3915e-02,  7.0084e-02,  3.8046e-02,  3.4062e-02,\n",
      "          4.3281e-02, -4.3885e-02, -1.3486e-02, -4.0698e-03,  1.4100e-02,\n",
      "         -3.8635e-02,  4.5243e-02, -2.7290e-02,  4.8370e-02, -1.4773e-02,\n",
      "          1.0768e-02,  3.7874e-04,  4.3831e-02, -2.0849e-02, -1.0954e-02,\n",
      "         -6.0031e-02,  3.8260e-02,  3.5814e-02, -2.7764e-02,  3.4530e-02,\n",
      "          1.1589e-03, -3.7993e-02, -1.0460e-02,  2.0098e-03,  1.8247e-02,\n",
      "          2.6382e-03, -6.6818e-03,  5.2589e-02,  1.8190e-02,  3.3213e-02,\n",
      "          4.5467e-03,  1.5070e-02, -2.8706e-02,  1.8993e-02,  2.1180e-02,\n",
      "         -2.0885e-03, -1.7697e-02,  5.5976e-02, -5.6691e-02,  1.4886e-03,\n",
      "         -1.7824e-02,  3.7113e-02, -2.6262e-02,  2.1573e-02,  2.7554e-03,\n",
      "          4.6666e-02,  1.9518e-02, -1.9964e-02, -2.6496e-02, -1.5558e-02,\n",
      "          2.8516e-02,  2.3572e-02,  1.3504e-02,  8.9583e-03,  1.7009e-02,\n",
      "         -1.4472e-02,  4.4917e-02, -1.8929e-02,  5.2749e-02,  1.7762e-02,\n",
      "         -5.3776e-02,  4.1798e-02, -5.1592e-03,  5.8070e-02, -2.4133e-02,\n",
      "         -2.1550e-02,  6.8855e-02,  1.5875e-02,  1.7735e-02, -4.5910e-02,\n",
      "         -7.4369e-02, -3.7779e-02, -3.6341e-02, -4.3445e-02, -9.3415e-02,\n",
      "          8.8673e-03,  1.4994e-02, -9.3193e-03,  7.5668e-02, -6.0180e-02,\n",
      "         -3.8880e-02,  7.0952e-02, -4.0360e-03, -1.5214e-02,  4.5774e-02,\n",
      "          5.9399e-02, -3.8463e-02, -7.7995e-03, -5.8877e-02,  3.8221e-02,\n",
      "          6.9705e-03, -5.5860e-02,  6.7781e-02,  4.1802e-02,  3.5873e-02,\n",
      "          3.1434e-03,  1.2452e-02,  7.3944e-03, -3.1515e-02,  3.3466e-02,\n",
      "          3.8334e-02, -2.1186e-02, -2.0521e-02, -3.6023e-02, -2.2704e-02,\n",
      "         -9.8215e-03,  8.6489e-03, -3.4091e-02, -2.0694e-02, -2.1112e-02,\n",
      "          1.7115e-02, -3.8038e-02, -3.6372e-02, -2.5891e-02, -3.3705e-02,\n",
      "         -1.7617e-02, -1.9382e-02, -9.1474e-02, -1.8461e-02,  4.4861e-02,\n",
      "         -3.8137e-02,  7.7448e-03, -3.2522e-02, -3.3014e-02,  2.9335e-02,\n",
      "          1.4200e-02, -2.6859e-02, -1.2675e-02,  2.0676e-03,  6.3773e-02,\n",
      "         -3.4888e-02,  3.9634e-03,  6.1769e-02,  6.2465e-02, -3.2971e-02,\n",
      "          8.4389e-04,  3.3232e-02, -4.7699e-02, -2.8073e-02, -2.2197e-02,\n",
      "         -2.7475e-02,  3.7666e-02, -9.5886e-03,  1.1666e-02,  2.0506e-02,\n",
      "          5.8219e-02, -1.3928e-02,  2.8729e-02,  1.8522e-02,  3.7457e-02,\n",
      "         -2.0841e-04, -2.8191e-02, -1.9606e-02,  5.4730e-02, -2.3712e-02,\n",
      "          2.2842e-02, -2.9731e-02, -1.4038e-02, -4.1129e-02,  9.3204e-03,\n",
      "          6.9563e-02,  1.8240e-02, -1.1978e-02,  9.2594e-03,  2.4660e-03,\n",
      "         -8.5112e-03,  6.1206e-03,  4.5549e-02, -2.4411e-02, -1.6399e-03,\n",
      "         -2.0789e-02, -6.1301e-02,  7.5525e-02, -5.0604e-02, -3.5039e-02,\n",
      "         -1.5663e-03, -1.2645e-02, -3.1869e-02,  1.7676e-02, -1.6956e-02,\n",
      "          1.1037e-02,  3.5036e-02, -4.5829e-04, -6.9227e-02, -4.3057e-02,\n",
      "          1.5379e-02,  1.5187e-02,  4.0742e-02, -3.3891e-02,  3.9983e-02,\n",
      "          2.8910e-02, -9.4966e-03,  2.6443e-02, -8.9153e-02, -3.1075e-02,\n",
      "         -2.6939e-02, -2.0384e-02, -2.4410e-02,  2.7851e-02, -2.7499e-03,\n",
      "          4.8215e-03, -2.7453e-02,  2.8747e-02,  3.6332e-02,  2.5731e-02,\n",
      "          3.2718e-02,  9.8570e-03, -1.2168e-02, -4.4953e-02, -2.0078e-03,\n",
      "          4.3978e-02,  4.8831e-02, -3.7453e-02, -6.6328e-02, -3.0357e-04,\n",
      "          5.8112e-02,  2.1000e-02, -1.8207e-02, -5.2756e-02, -3.0827e-02,\n",
      "          1.9485e-02,  1.0834e-02, -6.3215e-03,  4.2008e-02, -9.1125e-03,\n",
      "          4.8635e-03,  1.0690e-02, -2.0386e-02, -1.1442e-02, -1.4509e-03,\n",
      "         -2.3121e-02,  3.3906e-02,  7.0831e-03,  1.5312e-02,  3.0320e-02,\n",
      "          1.4876e-02,  6.2754e-03, -1.9871e-02,  3.1921e-02,  2.4657e-02,\n",
      "          3.9766e-02, -5.0256e-02,  6.4318e-03, -7.5621e-02,  3.8655e-02,\n",
      "         -4.6263e-02,  1.7899e-02,  9.1417e-03, -9.4321e-03, -1.4513e-02,\n",
      "          3.7495e-02,  2.0129e-02, -1.5485e-02, -3.1912e-02, -8.8849e-02,\n",
      "         -9.5425e-03, -5.0247e-02, -3.9575e-02, -3.1759e-02, -1.1398e-02,\n",
      "          9.0362e-04,  6.5283e-02,  2.7319e-02, -2.9547e-03,  2.2056e-02,\n",
      "         -6.6268e-02, -1.6344e-02,  7.7697e-02,  3.6122e-02, -1.8254e-03,\n",
      "         -2.2998e-02, -3.3850e-02,  4.2020e-02, -3.7569e-02, -9.5670e-03,\n",
      "          2.3507e-02, -1.6802e-02, -8.2791e-03, -3.4590e-03, -8.6334e-03,\n",
      "         -1.5853e-02,  2.1162e-02, -1.9197e-02,  1.6781e-02, -5.8069e-02,\n",
      "          4.1461e-02, -1.6077e-02,  6.4990e-02, -2.9171e-02, -4.1446e-02,\n",
      "         -2.5188e-02, -2.6257e-02, -1.0691e-03, -1.6593e-03, -2.5618e-02,\n",
      "          3.0601e-02,  3.0040e-02,  1.4387e-02,  1.7191e-02,  1.1375e-03,\n",
      "         -1.0793e-02, -1.9950e-02,  1.2344e-02,  9.8023e-02, -6.2723e-02,\n",
      "         -2.7159e-02, -3.8165e-03,  2.4426e-02,  1.9734e-02, -2.2794e-02,\n",
      "          7.8796e-02,  3.2980e-02,  2.4735e-02, -1.9048e-02, -2.7875e-03,\n",
      "         -5.5359e-05,  4.0559e-03,  2.5404e-02, -3.2556e-02,  2.7580e-03,\n",
      "          3.3692e-02,  6.4968e-02, -4.0957e-02,  8.8045e-02, -4.9562e-02,\n",
      "          2.0951e-02, -6.2206e-02, -1.6343e-02,  5.6901e-02, -8.2545e-03,\n",
      "         -3.7659e-02, -4.9165e-03, -4.4819e-02,  4.7290e-03, -5.3496e-03,\n",
      "          7.5031e-02,  5.9098e-02, -3.1653e-02, -1.3582e-02,  1.0048e-02,\n",
      "         -6.9399e-02, -7.0507e-03, -8.6135e-03, -1.9423e-02,  4.6720e-02,\n",
      "          5.3132e-02, -2.3422e-02,  5.9839e-02,  2.7007e-02, -2.7640e-02,\n",
      "         -8.5471e-03, -6.1931e-02,  5.5341e-02,  8.8063e-02, -4.8722e-02,\n",
      "          5.6986e-02, -2.2693e-02, -9.3901e-03, -8.8392e-03,  4.6762e-03,\n",
      "         -9.3090e-03, -1.2362e-02, -2.1525e-02,  1.7384e-03, -5.9423e-02,\n",
      "          2.8296e-02,  1.9929e-02, -5.1992e-02, -3.5789e-03,  4.2592e-02,\n",
      "          3.6169e-02, -2.2059e-02, -2.5655e-02, -5.2815e-03,  7.0405e-02,\n",
      "          1.0306e-02, -2.0120e-02, -1.2484e-02, -1.8428e-02,  1.6978e-02,\n",
      "         -3.4156e-04,  2.7648e-02,  1.3002e-02, -6.3190e-03,  4.3923e-02,\n",
      "         -2.2831e-02, -6.3054e-03, -1.9887e-02,  1.3736e-02, -1.2402e-02,\n",
      "          1.1904e-03, -8.2146e-03,  2.7281e-03,  3.5535e-02,  3.0684e-02,\n",
      "          7.1365e-02, -3.1957e-02,  2.4540e-02,  2.4862e-02,  4.1273e-02,\n",
      "         -3.7000e-02, -4.5526e-02, -6.8416e-04, -1.9430e-02,  6.1212e-02,\n",
      "          3.0250e-02,  1.4622e-02, -1.2605e-02,  1.1943e-02,  8.0883e-02,\n",
      "         -4.1759e-02,  9.0031e-03, -2.5871e-02, -2.5671e-02,  3.5844e-02,\n",
      "          6.1176e-02, -7.5647e-03, -2.3530e-02, -3.1647e-02, -2.0243e-02,\n",
      "         -3.7247e-02,  3.9476e-02, -1.0296e-02,  1.0930e-04, -3.5810e-02,\n",
      "         -4.1114e-02,  9.3048e-03,  5.5796e-02, -3.8897e-03, -1.9391e-02,\n",
      "         -4.1673e-02,  8.1645e-02, -6.2742e-02, -8.6719e-02, -6.2871e-02,\n",
      "          5.3393e-02, -2.1418e-02,  4.2849e-04, -8.5592e-03,  7.9945e-03,\n",
      "          2.0900e-02,  3.6050e-02, -3.0677e-02,  1.3211e-04, -4.0572e-02,\n",
      "          3.5856e-02, -2.9317e-02,  6.4852e-02, -3.2392e-02, -8.2943e-03,\n",
      "          1.6393e-02,  6.8345e-02,  7.9238e-02,  2.2026e-02, -1.6467e-02,\n",
      "         -2.6900e-02, -2.1440e-02, -1.7878e-03,  1.7492e-02, -4.2632e-02,\n",
      "         -4.1093e-02,  6.3237e-03, -2.2788e-03,  3.8470e-03, -7.5330e-02,\n",
      "          2.8123e-02,  5.1723e-02, -3.7322e-02,  7.7535e-02,  1.2608e-02,\n",
      "          3.1640e-02, -2.0024e-02, -4.7228e-02,  5.9564e-02,  4.0928e-02,\n",
      "          6.6765e-02, -3.9459e-02, -8.3590e-02,  5.9641e-02, -7.0208e-02,\n",
      "         -4.8185e-02, -1.9527e-02, -1.3413e-02, -6.0620e-02, -1.8747e-03,\n",
      "         -5.8647e-02,  5.3908e-03, -5.9117e-03, -2.2430e-02, -2.1444e-02,\n",
      "          1.4519e-02, -6.4302e-02,  4.5887e-02,  2.3313e-02,  9.4305e-04,\n",
      "          3.6830e-02, -4.6056e-02,  5.4562e-02, -7.9591e-03,  7.8654e-02,\n",
      "         -6.8919e-02, -1.0548e-02, -2.2805e-02,  4.4468e-02, -1.9169e-02,\n",
      "         -6.6173e-02,  4.9209e-02, -1.3015e-01]])\n",
      "prompt_text_feat tensor([[-0.0534, -0.0227, -0.0297, -0.0426, -0.0151, -0.0294, -0.0299,  0.0201,\n",
      "         -0.0091,  0.0176, -0.0103, -0.0262,  0.0126, -0.0120, -0.0059,  0.0334,\n",
      "          0.0109,  0.0029, -0.0151, -0.0046, -0.0785, -0.0077,  0.0023,  0.0322,\n",
      "          0.0467, -0.0309,  0.0051, -0.0372, -0.0058,  0.0736, -0.0383,  0.0183,\n",
      "          0.0258,  0.0452, -0.0234,  0.0493, -0.0392, -0.0151,  0.0123,  0.0195,\n",
      "         -0.0502,  0.0513, -0.0017,  0.0412,  0.0402, -0.0038,  0.0162, -0.0236,\n",
      "         -0.0154,  0.0358, -0.0594,  0.0266, -0.0497,  0.0184,  0.0013, -0.0639,\n",
      "         -0.0041,  0.0042, -0.0084,  0.0295, -0.0554,  0.0021,  0.0129,  0.0502,\n",
      "          0.0802,  0.0452,  0.0169, -0.0174, -0.0811,  0.0033,  0.0450, -0.0221,\n",
      "          0.0279,  0.0065,  0.0308, -0.0482,  0.0122,  0.0402, -0.0266,  0.0009,\n",
      "         -0.0076,  0.0194, -0.0021,  0.0294, -0.0161,  0.0373, -0.0367, -0.0487,\n",
      "          0.0170, -0.0370, -0.0326,  0.0180, -0.0379, -0.0028,  0.0137, -0.0088,\n",
      "         -0.0177, -0.0684,  0.0105, -0.0059,  0.0317, -0.0323, -0.0071,  0.0023,\n",
      "         -0.0529,  0.0003,  0.0401, -0.0402, -0.0009,  0.0659,  0.0138, -0.0213,\n",
      "          0.0464,  0.0384,  0.0140,  0.0623,  0.0125, -0.0215, -0.0024, -0.0129,\n",
      "         -0.0071,  0.0302, -0.0160,  0.0030, -0.0255,  0.0370,  0.0426,  0.0082,\n",
      "          0.0177, -0.0117,  0.0213, -0.0073, -0.0191,  0.0130, -0.0188,  0.0061,\n",
      "          0.0012,  0.0211,  0.0296, -0.0017, -0.0345, -0.0385, -0.0182, -0.0643,\n",
      "          0.0093, -0.0052, -0.0008,  0.0204,  0.0233, -0.1065,  0.0439, -0.0129,\n",
      "         -0.0418,  0.0198, -0.0756,  0.0347,  0.0083,  0.0489,  0.0428,  0.0198,\n",
      "          0.0013, -0.0255, -0.0329,  0.0566, -0.0078, -0.0126, -0.0228, -0.0233,\n",
      "          0.0155, -0.0157,  0.0489, -0.0516, -0.0230, -0.0129, -0.0193, -0.0247,\n",
      "         -0.0013,  0.0149, -0.0335,  0.0159, -0.0562,  0.0697, -0.0048, -0.0286,\n",
      "         -0.0159, -0.0154,  0.0060,  0.0228, -0.0253,  0.0218,  0.0078,  0.0804,\n",
      "          0.0224,  0.0168,  0.0161,  0.0298,  0.0394,  0.0480, -0.1079, -0.0298,\n",
      "          0.0583,  0.0163,  0.0009, -0.0014, -0.0458, -0.0050, -0.0262, -0.0104,\n",
      "          0.0544, -0.0218, -0.0427,  0.0888, -0.0284,  0.0126,  0.0678,  0.0283,\n",
      "          0.0487, -0.0027, -0.0226, -0.0689,  0.0474,  0.0484,  0.0788, -0.0103,\n",
      "         -0.0048,  0.0170,  0.0322,  0.0025,  0.0515, -0.0095, -0.0104,  0.0197,\n",
      "          0.0453,  0.0224,  0.0122, -0.0075, -0.0414,  0.0123,  0.0269, -0.0152,\n",
      "          0.0298, -0.0625, -0.0984, -0.0211,  0.0684,  0.0219,  0.0032,  0.0336,\n",
      "         -0.0178,  0.0233, -0.0304,  0.0522,  0.0087, -0.0815, -0.0092,  0.0211,\n",
      "         -0.0445, -0.0012,  0.0177,  0.0284,  0.0729, -0.0621, -0.0531, -0.0010,\n",
      "         -0.0135, -0.0568,  0.0294, -0.0264,  0.0553,  0.0332,  0.0394, -0.0027,\n",
      "          0.0726, -0.0391, -0.0357, -0.0505,  0.0306,  0.0231,  0.0275,  0.0279,\n",
      "         -0.0798, -0.0513,  0.0020,  0.0023,  0.0624,  0.0796, -0.0344, -0.0266,\n",
      "          0.0271,  0.0160,  0.0386,  0.0270, -0.0295,  0.0356,  0.0390, -0.0422,\n",
      "         -0.0064,  0.0241,  0.0152, -0.0175, -0.0272,  0.0672, -0.0275,  0.0440,\n",
      "         -0.0360,  0.0996,  0.0473, -0.0003, -0.0095, -0.0470,  0.0321,  0.0461,\n",
      "         -0.0152,  0.0278, -0.0268,  0.0446,  0.0448,  0.0096, -0.0398,  0.0247,\n",
      "         -0.0134,  0.0358,  0.0402,  0.0260, -0.0703, -0.0362,  0.0030, -0.0151,\n",
      "         -0.0537, -0.0186, -0.0149, -0.0428, -0.0367, -0.0371, -0.0226,  0.0111,\n",
      "          0.0565, -0.0609,  0.0041, -0.0799, -0.0240,  0.0355,  0.0180, -0.0128,\n",
      "          0.0235, -0.0098, -0.0491,  0.0142, -0.0436, -0.0498,  0.0154, -0.0077,\n",
      "          0.0555,  0.0140,  0.0409, -0.0048, -0.0108, -0.0360,  0.0201,  0.0321,\n",
      "          0.0126,  0.0653,  0.0016, -0.0066, -0.0515, -0.0119, -0.0506, -0.0235,\n",
      "         -0.0129,  0.0171, -0.0222, -0.0263, -0.0473, -0.0417, -0.0809, -0.0127,\n",
      "         -0.0264, -0.1029,  0.0157,  0.0175, -0.0053, -0.0100, -0.0218, -0.0256,\n",
      "         -0.0164,  0.0724, -0.0115,  0.0422,  0.0149,  0.0197, -0.0303, -0.0359,\n",
      "          0.0099, -0.0102,  0.0533, -0.0410,  0.0307,  0.0233, -0.0140,  0.0767,\n",
      "         -0.0544,  0.0093, -0.0318,  0.0421, -0.0135, -0.0205, -0.0025, -0.0289,\n",
      "          0.0285,  0.0593, -0.0386, -0.0233, -0.0411, -0.0370, -0.0753, -0.0127,\n",
      "          0.0520, -0.0385, -0.0011, -0.0120,  0.0558,  0.0081,  0.0005,  0.0657,\n",
      "         -0.0274, -0.0028,  0.0217, -0.0303,  0.0096,  0.0095,  0.0845, -0.0043,\n",
      "          0.0318, -0.0417, -0.0219, -0.0155, -0.0203, -0.0086,  0.0045, -0.0146,\n",
      "         -0.0063,  0.0436,  0.0314,  0.0759, -0.0149,  0.0414,  0.0227,  0.0214,\n",
      "         -0.0648,  0.0194,  0.0165, -0.0232,  0.0757, -0.0805, -0.0558, -0.0242,\n",
      "          0.0160, -0.0070, -0.0090, -0.0368,  0.0423, -0.0657,  0.0241, -0.0063,\n",
      "         -0.0070, -0.0344,  0.0360, -0.0087, -0.0067,  0.0198,  0.0744, -0.0144,\n",
      "          0.0067, -0.0318,  0.0310,  0.0448, -0.0052, -0.0308, -0.0501,  0.0007,\n",
      "          0.0319, -0.0133, -0.0060, -0.0335,  0.0129,  0.0379, -0.0457, -0.0181,\n",
      "         -0.0149, -0.0369, -0.0407,  0.0585,  0.0476, -0.0027,  0.0558,  0.0185,\n",
      "          0.0061, -0.0119, -0.0363,  0.0224,  0.0577, -0.0133, -0.0151, -0.0403,\n",
      "          0.0058, -0.0376, -0.0061, -0.0221, -0.0110, -0.0075,  0.0115, -0.0084,\n",
      "          0.0186, -0.0400, -0.0549,  0.0233, -0.0412, -0.0426,  0.0013,  0.0082,\n",
      "          0.0123,  0.0248,  0.0222,  0.0130, -0.0201, -0.0849,  0.0006,  0.0518,\n",
      "          0.0025,  0.0404,  0.0020, -0.0456,  0.0398,  0.0053, -0.0138, -0.0166,\n",
      "         -0.0122,  0.0533,  0.0166,  0.0402,  0.0164, -0.0294,  0.0300, -0.0216,\n",
      "         -0.0112,  0.0407, -0.0016,  0.0829, -0.0241, -0.0079, -0.0319, -0.0395,\n",
      "          0.0417, -0.0056,  0.0155, -0.0141, -0.0394,  0.0063,  0.0175,  0.0047,\n",
      "         -0.0317,  0.0222,  0.0589,  0.0422, -0.0199,  0.0229,  0.0249, -0.0020,\n",
      "          0.0056,  0.0120,  0.0380,  0.0147, -0.0157, -0.0019, -0.0389, -0.0397,\n",
      "         -0.0397,  0.0273, -0.0104,  0.0258,  0.0353,  0.0019,  0.0182,  0.0327,\n",
      "         -0.0336, -0.0100, -0.0797,  0.0276,  0.0089,  0.0052, -0.0135,  0.0271,\n",
      "         -0.0235,  0.0107,  0.0033,  0.0293,  0.0354, -0.0412, -0.0401, -0.0161,\n",
      "         -0.0415,  0.0107, -0.0125,  0.0077,  0.0225,  0.0647, -0.0264, -0.0140,\n",
      "         -0.0138, -0.0133,  0.0966, -0.0678,  0.0038,  0.0655, -0.0211,  0.0344,\n",
      "         -0.0165, -0.0257, -0.0423, -0.0330, -0.0366, -0.0620,  0.0057,  0.0573,\n",
      "         -0.0120, -0.0827,  0.0647, -0.0315, -0.0030,  0.0199, -0.0107, -0.0048,\n",
      "         -0.0478,  0.0254, -0.0883, -0.0073, -0.0330,  0.0037,  0.0515,  0.0646,\n",
      "         -0.0148,  0.0223, -0.0360,  0.0106,  0.0178,  0.0297, -0.0078,  0.0281,\n",
      "          0.0182,  0.0326,  0.0059,  0.0052, -0.0044,  0.0149,  0.0174,  0.0752,\n",
      "          0.0109,  0.0689, -0.0171, -0.0017, -0.0593,  0.0384,  0.0138, -0.0272,\n",
      "         -0.0112,  0.0038, -0.0201,  0.0387, -0.0071,  0.0957, -0.0175,  0.0722,\n",
      "         -0.0657,  0.0125,  0.0232,  0.0209,  0.0097, -0.0580,  0.0571, -0.0316,\n",
      "         -0.0237,  0.0764,  0.0070, -0.0147,  0.0201, -0.0107, -0.0263,  0.0372,\n",
      "         -0.0774, -0.0514,  0.0088,  0.0512, -0.0540, -0.0635, -0.0733,  0.0218,\n",
      "         -0.0188,  0.0580, -0.0388,  0.0369,  0.0251,  0.0199, -0.0127,  0.0728,\n",
      "         -0.0075, -0.0041, -0.0079, -0.0034, -0.0531, -0.0363,  0.0457,  0.0332,\n",
      "          0.0350,  0.0634,  0.0221, -0.0178, -0.0373, -0.0443,  0.0122,  0.0066,\n",
      "         -0.0062, -0.0142, -0.0035,  0.0523, -0.0743,  0.0028,  0.0654, -0.0418,\n",
      "          0.0512, -0.0455,  0.0118,  0.0253,  0.0072,  0.0177,  0.0310, -0.0202,\n",
      "         -0.0323, -0.0334,  0.0352, -0.0847, -0.0313,  0.0018, -0.0041, -0.0319,\n",
      "          0.0082,  0.0006,  0.0118, -0.0108, -0.0282, -0.0279,  0.0475, -0.0061,\n",
      "          0.0357,  0.0608,  0.0217,  0.0384, -0.0294,  0.0040, -0.0216,  0.0685,\n",
      "         -0.0198,  0.0196, -0.0538,  0.0611, -0.0705, -0.0381,  0.0267, -0.0250]])\n",
      "prompt_edge_feat tensor([[-6.4458e-02, -8.1989e-02,  8.5701e-04,  9.6073e-03,  1.1524e-02,\n",
      "          1.6484e-02,  4.7118e-03,  4.5429e-02,  6.5905e-02, -7.3102e-02,\n",
      "         -1.1490e-02,  7.3549e-03,  1.1927e-02, -2.2291e-02,  1.7271e-02,\n",
      "          2.5840e-02, -2.4100e-02, -4.4611e-04, -3.1512e-02, -2.5258e-02,\n",
      "         -1.9750e-02, -3.2841e-02,  2.8579e-03, -1.4594e-02,  5.3424e-02,\n",
      "         -4.2091e-03, -1.5439e-02, -9.3944e-02, -2.2702e-02,  4.6457e-03,\n",
      "         -3.9696e-02,  1.4148e-02,  1.4398e-02, -7.9221e-03,  5.6800e-03,\n",
      "         -3.6887e-02, -2.2812e-02,  7.1459e-02, -3.0558e-02,  4.1924e-02,\n",
      "         -9.0045e-02,  2.2337e-02,  4.8169e-02,  3.1953e-02,  4.2822e-02,\n",
      "         -6.8657e-02,  1.2373e-02,  3.4414e-02,  9.8277e-03, -3.7034e-02,\n",
      "         -2.9342e-02,  5.2162e-02, -1.5424e-02,  1.0366e-02,  1.8561e-02,\n",
      "         -5.8350e-02,  3.0935e-02, -2.0195e-02,  1.1563e-02,  5.1891e-03,\n",
      "         -3.0158e-02,  2.5030e-02,  3.9713e-02,  3.4863e-02, -6.6416e-02,\n",
      "          2.0803e-02,  4.2653e-03,  1.5580e-02, -2.5159e-02, -4.2544e-02,\n",
      "          4.8138e-02,  2.7725e-02,  3.9996e-02, -1.9475e-02,  7.8293e-02,\n",
      "          2.6855e-02,  1.4690e-02,  4.3660e-02, -6.5333e-03,  4.6482e-04,\n",
      "         -5.8710e-03, -2.1243e-02,  2.3547e-02,  3.8535e-02, -2.3207e-03,\n",
      "         -1.3496e-02,  4.8254e-03,  2.9291e-03,  1.0017e-02, -2.5755e-02,\n",
      "         -4.2182e-02,  6.5610e-03, -6.1420e-02, -1.3492e-02, -3.3416e-03,\n",
      "          1.5365e-02,  2.4028e-02, -4.0776e-03,  2.7855e-02,  6.4863e-02,\n",
      "         -5.8379e-02, -1.0703e-01, -1.7126e-02, -1.7713e-03, -6.9873e-03,\n",
      "         -3.5021e-02, -2.9691e-03,  2.1163e-02, -1.1230e-02,  5.2672e-02,\n",
      "         -2.8054e-02, -2.0834e-02, -9.3766e-03,  4.3817e-02,  5.2626e-02,\n",
      "          6.7160e-02,  1.3149e-02,  1.1190e-02, -3.1670e-03, -5.4208e-02,\n",
      "         -7.0519e-02,  2.1548e-02, -1.3680e-02,  4.3183e-02,  9.2549e-03,\n",
      "         -2.6469e-03,  5.1386e-03,  4.6319e-02,  1.0731e-02,  1.1460e-02,\n",
      "          1.3790e-02,  5.1218e-02,  8.6061e-03, -3.0741e-04, -3.9337e-02,\n",
      "          2.0205e-02,  5.3514e-02, -5.7423e-03, -8.5489e-03, -1.2020e-02,\n",
      "          2.9902e-02,  1.4281e-02,  4.0584e-02, -5.2172e-03,  4.4144e-02,\n",
      "         -8.1953e-03,  6.2726e-02,  2.3834e-02, -1.6902e-02,  2.9468e-02,\n",
      "          2.4583e-03, -1.4035e-02,  6.0515e-03,  3.0234e-02, -2.8786e-02,\n",
      "         -6.4725e-03, -1.2794e-03, -1.8302e-02, -1.2616e-04, -3.1191e-02,\n",
      "          1.0691e-02,  4.9526e-02,  3.3734e-02,  5.3887e-03,  4.6335e-02,\n",
      "          2.5853e-02, -3.7806e-04, -1.1227e-02, -4.3319e-02,  1.1675e-02,\n",
      "          2.5807e-02,  3.7064e-02, -1.8427e-02,  2.9829e-02,  4.3539e-02,\n",
      "         -5.7764e-03,  4.6469e-02,  2.4666e-02,  4.7626e-02, -5.6202e-03,\n",
      "         -4.9069e-02,  1.7004e-02,  3.1720e-02,  1.6762e-04, -5.0063e-02,\n",
      "         -2.5601e-02,  2.1388e-02, -5.5696e-03, -6.5500e-02, -3.6487e-02,\n",
      "         -3.9249e-03, -6.2998e-03, -7.3947e-02,  4.7363e-04,  7.8533e-02,\n",
      "         -5.0279e-02,  3.7784e-02,  2.0864e-02, -4.7904e-02,  3.4722e-02,\n",
      "          4.0794e-02, -1.5279e-02,  2.5295e-02, -3.7314e-02, -3.8833e-02,\n",
      "          2.9213e-02,  4.5332e-02, -2.0283e-02, -3.3457e-02,  3.2502e-02,\n",
      "         -7.1335e-02, -2.0736e-02,  2.4277e-03,  9.1286e-02,  4.1924e-02,\n",
      "          3.8027e-02,  3.3690e-02, -7.6237e-02,  3.5192e-02, -6.3353e-02,\n",
      "         -3.8149e-02,  4.2643e-02, -4.3034e-03,  3.6615e-02,  2.8815e-02,\n",
      "          3.7953e-02,  2.4917e-02,  3.1907e-02,  2.9684e-02, -3.7262e-02,\n",
      "         -7.8761e-02, -8.4440e-04,  9.7982e-03, -4.8850e-03,  2.0450e-02,\n",
      "         -1.7804e-02,  3.9278e-02, -3.6249e-02,  1.8110e-02, -1.9723e-02,\n",
      "          2.1533e-02, -5.9784e-02,  6.3114e-02, -3.6081e-02,  3.7972e-02,\n",
      "          1.6048e-03,  1.3877e-02,  2.4550e-02,  3.1804e-02,  1.8598e-02,\n",
      "          9.8480e-04,  9.8077e-02,  1.0856e-02, -7.6805e-02, -6.0150e-02,\n",
      "          6.4088e-02,  7.4073e-03, -5.8129e-02,  5.8549e-03,  4.9025e-02,\n",
      "          6.4851e-02,  6.1271e-03, -3.3686e-02,  1.6313e-02,  3.0326e-03,\n",
      "         -2.4368e-02, -2.1382e-02,  2.2373e-02,  8.3286e-02,  2.8033e-02,\n",
      "          1.6981e-03,  4.9798e-02, -8.6874e-03, -4.3946e-02, -5.8485e-02,\n",
      "         -9.4238e-02,  3.9031e-02,  6.5628e-02,  5.8088e-04, -3.6915e-02,\n",
      "          1.8229e-02, -3.3492e-02, -2.1328e-02,  5.2650e-02,  5.5868e-02,\n",
      "         -1.0409e-02,  8.4327e-02, -7.1570e-02, -8.6566e-02, -1.7744e-02,\n",
      "         -1.3538e-02,  2.1343e-02, -1.5022e-02,  3.9339e-02,  3.9028e-02,\n",
      "          2.1151e-03,  5.3476e-02, -1.0934e-03,  4.9624e-02, -7.4183e-04,\n",
      "          8.6112e-02, -7.8903e-03,  1.7285e-02, -1.7095e-02,  2.2450e-02,\n",
      "         -1.2669e-02,  5.6111e-04, -7.1916e-02, -2.8002e-02, -2.9755e-02,\n",
      "          7.1149e-02, -3.3975e-02, -2.6254e-02, -1.8058e-02, -9.5500e-03,\n",
      "          3.1252e-02,  2.3439e-02, -1.8105e-02,  1.4013e-02,  3.0364e-02,\n",
      "         -2.5685e-02, -3.1695e-02,  1.7623e-02,  2.9681e-02,  7.4311e-02,\n",
      "         -1.1663e-01,  8.9092e-03, -5.5359e-02, -7.8186e-03, -3.0147e-02,\n",
      "          4.9356e-02,  5.6649e-02, -4.4040e-03,  2.3565e-02,  1.3929e-02,\n",
      "         -2.4189e-02,  4.1279e-02, -9.3453e-03,  5.3557e-02, -2.6162e-02,\n",
      "         -3.4425e-02, -5.7106e-03,  2.3150e-02, -1.2565e-02,  1.1708e-02,\n",
      "         -1.2910e-02, -6.7828e-03, -9.5800e-03,  3.3725e-02,  2.5382e-02,\n",
      "          2.5356e-02, -7.7357e-02,  3.0291e-03, -2.5422e-02,  3.4069e-02,\n",
      "         -6.8303e-02,  1.1465e-02,  5.3078e-02, -1.0999e-02,  3.7530e-02,\n",
      "         -2.4283e-02,  1.0893e-02,  2.7417e-02,  6.0165e-02, -1.5535e-02,\n",
      "         -2.2596e-02, -7.5311e-02, -2.4643e-02, -3.3780e-03, -3.4717e-02,\n",
      "         -3.1205e-02, -6.2875e-02,  6.3914e-02, -4.2283e-02, -5.4954e-02,\n",
      "         -3.6714e-02, -4.3721e-02, -5.7881e-02,  8.8467e-04, -1.1909e-02,\n",
      "         -2.7575e-02, -1.7942e-02,  1.3891e-03, -1.3236e-02,  2.6223e-02,\n",
      "         -1.3048e-03,  4.9403e-03, -3.0269e-02, -8.4306e-02,  1.9976e-02,\n",
      "          3.1127e-02,  4.1949e-02, -3.2253e-02,  5.3697e-04,  4.3845e-02,\n",
      "         -4.2391e-02, -6.7463e-03,  1.3147e-02, -7.4768e-03,  5.2929e-02,\n",
      "         -5.7655e-02, -4.2552e-02,  3.6557e-02,  3.1957e-02,  5.6678e-02,\n",
      "         -5.6788e-03, -1.4497e-02, -3.9882e-02,  1.9651e-02,  2.4522e-02,\n",
      "          3.3282e-02, -2.4788e-02, -3.4463e-02,  7.2783e-03, -3.3816e-03,\n",
      "         -5.5520e-02,  1.4525e-02, -1.4352e-02,  1.3444e-02, -5.6286e-02,\n",
      "         -3.6468e-02, -1.6125e-02, -2.9771e-02,  2.7220e-03, -3.9213e-03,\n",
      "         -1.3026e-02,  5.0751e-04, -2.4477e-02, -2.0593e-02,  6.4767e-02,\n",
      "          5.6608e-03,  4.5406e-02,  7.3801e-02, -5.8504e-02, -3.2776e-02,\n",
      "          2.0005e-02,  5.8903e-02,  1.6608e-02,  1.6521e-02, -1.3127e-02,\n",
      "         -4.6655e-03, -1.4569e-02,  5.2171e-02, -1.6464e-02,  3.7597e-02,\n",
      "         -2.7255e-02,  3.3310e-03,  9.4144e-02, -4.7111e-02,  1.1549e-02,\n",
      "         -4.6275e-02, -1.9728e-02,  1.2733e-01, -1.1989e-03, -4.0735e-02,\n",
      "         -2.0146e-02,  2.1228e-02,  8.3682e-03,  4.0615e-03,  8.6286e-03,\n",
      "          2.6567e-02, -4.1650e-02,  6.7524e-02,  1.4719e-02,  3.0029e-02,\n",
      "         -5.4238e-02, -3.4971e-02,  2.7789e-02,  3.0370e-04,  3.2451e-02,\n",
      "          3.6519e-03,  3.9268e-02,  2.2540e-02, -2.3703e-02,  1.7244e-02,\n",
      "         -2.8196e-02, -5.1800e-03, -7.4764e-02, -9.3603e-03,  3.7074e-03,\n",
      "          6.2021e-02,  7.9969e-02,  3.1336e-03, -4.1918e-03, -3.4209e-02,\n",
      "         -2.9032e-02, -1.3122e-03, -2.0712e-02,  1.9265e-02, -2.1436e-02,\n",
      "          2.9771e-03,  5.1944e-02, -3.3902e-02, -1.1674e-02, -1.1628e-02,\n",
      "          3.1594e-02, -2.0004e-03,  1.4299e-02, -5.7028e-02, -3.2226e-02,\n",
      "         -4.2320e-02, -2.4666e-02, -1.7861e-02, -1.4743e-02, -3.0465e-02,\n",
      "         -3.1915e-02,  2.8901e-02, -9.5762e-03, -2.9847e-02, -4.2564e-02,\n",
      "          9.6604e-03,  2.6603e-02,  2.2282e-02,  3.3158e-02, -5.3145e-02,\n",
      "         -3.1738e-03,  2.9674e-02, -8.2194e-03, -2.6520e-02, -2.2277e-02,\n",
      "         -9.2121e-03, -6.5557e-02,  4.1763e-03, -6.1948e-03, -2.5393e-02,\n",
      "         -4.8903e-02, -3.1193e-02,  2.8346e-02,  9.3376e-03, -3.3963e-02,\n",
      "         -4.6253e-03,  3.4530e-02, -3.4537e-02,  1.9563e-02,  5.6909e-03,\n",
      "         -1.7898e-02,  1.8911e-02,  4.7272e-02, -6.5254e-02,  2.7337e-02,\n",
      "         -1.7625e-02, -2.7661e-03,  3.6578e-02, -2.0254e-02, -7.8151e-02,\n",
      "          1.9283e-03, -2.9028e-02,  3.7696e-02, -9.9931e-03, -7.2460e-02,\n",
      "         -2.6900e-02, -4.8114e-02,  6.4867e-03,  2.7740e-02, -3.7351e-02,\n",
      "         -2.1214e-02,  1.1686e-02,  7.1042e-02,  1.7446e-03, -2.3725e-03,\n",
      "          2.0739e-02, -1.2525e-02, -4.7079e-02,  6.2554e-03,  1.0106e-02,\n",
      "          6.0356e-02, -1.2129e-02,  5.5194e-02, -1.8363e-02, -1.1702e-01,\n",
      "          2.0139e-02,  1.7261e-02,  2.7075e-02,  2.8355e-03, -1.5383e-02,\n",
      "          2.3889e-02, -3.8948e-02, -8.3757e-04, -3.7797e-03, -2.1020e-03,\n",
      "          5.8162e-02, -3.5064e-02, -4.9785e-02, -3.0187e-02, -2.8365e-02,\n",
      "          3.9451e-02, -3.7400e-02, -4.2427e-04,  1.0391e-02,  3.5456e-03,\n",
      "         -2.7467e-02,  3.4384e-02, -6.7697e-02, -3.1741e-02, -4.0009e-03,\n",
      "         -2.5724e-02,  2.1648e-02,  2.0303e-02, -2.1203e-02,  5.2676e-02,\n",
      "         -2.8838e-02,  3.3278e-04, -1.7842e-02,  5.3799e-03,  5.5144e-02,\n",
      "          4.0306e-02, -1.0694e-02,  4.1139e-02,  3.8396e-02,  9.8594e-03,\n",
      "         -3.6456e-02, -1.8739e-02, -6.1277e-03,  2.9446e-02,  2.9579e-03,\n",
      "         -1.0077e-02, -2.6899e-02, -1.0003e-02, -1.7871e-02, -2.3666e-02,\n",
      "          2.0237e-02, -5.4364e-03,  6.4678e-02, -1.4579e-02,  5.3201e-02,\n",
      "         -3.5700e-02, -6.5435e-02, -1.0626e-02, -3.6689e-02,  6.8126e-02,\n",
      "         -6.9063e-03,  2.4396e-02, -1.3647e-02, -3.8702e-03,  2.1757e-02,\n",
      "          7.3423e-03,  4.5487e-02,  5.0528e-02, -3.8777e-03,  6.0875e-02,\n",
      "          5.3342e-02, -3.2534e-02, -3.0606e-02, -2.1319e-02,  2.0512e-02,\n",
      "          2.7096e-03,  1.9043e-02,  3.5276e-03,  1.9082e-02,  3.1156e-02,\n",
      "         -2.5699e-02, -5.6925e-03, -1.5720e-02,  6.5113e-02,  4.7188e-02,\n",
      "          3.3029e-02, -3.3623e-02,  7.4230e-02,  6.3737e-03, -1.0543e-02,\n",
      "         -7.0239e-02,  1.9009e-02,  3.0270e-02, -3.1222e-02,  4.1108e-03,\n",
      "          2.9479e-02,  2.6732e-02,  5.2488e-02, -3.9180e-02, -1.6746e-03,\n",
      "          6.1512e-02, -5.0707e-02, -5.0654e-02,  6.5465e-02, -3.9274e-02,\n",
      "          1.8536e-02, -3.7011e-02,  4.8012e-02,  5.8278e-02, -1.6763e-02,\n",
      "          5.8082e-02,  1.1895e-02, -1.4337e-02, -6.1629e-03, -1.0098e-02,\n",
      "          1.2106e-02, -1.9678e-02,  2.3014e-02, -1.3072e-02, -3.7858e-03,\n",
      "         -4.0266e-02,  5.5647e-03,  2.1013e-02, -1.9287e-02, -3.5002e-02,\n",
      "          8.8274e-03,  2.2415e-02, -2.1575e-02, -2.0811e-02, -6.4378e-03,\n",
      "          6.7450e-02, -3.0676e-02,  6.8976e-02, -2.0603e-02,  1.4900e-02,\n",
      "          5.3484e-02,  4.9998e-03,  5.5771e-02,  1.9284e-03, -2.3079e-02,\n",
      "          1.9125e-02, -6.0572e-02,  1.9854e-02,  1.3580e-02, -2.1918e-03,\n",
      "         -2.4816e-02,  3.2151e-02, -2.6221e-02,  6.4538e-02,  1.0085e-02,\n",
      "         -4.1602e-02, -5.5897e-03, -8.2895e-02,  1.9726e-03,  3.2683e-03,\n",
      "          3.8568e-02,  3.1006e-02, -8.2364e-02,  2.5520e-02, -7.6797e-04,\n",
      "          3.9261e-03,  1.4076e-02, -1.6570e-02, -6.5828e-03,  4.9225e-02,\n",
      "         -1.2150e-02, -3.8507e-04,  1.6217e-02, -9.2516e-03, -7.7827e-02,\n",
      "          3.3329e-02,  5.2237e-03,  3.0954e-02, -2.6379e-02,  2.4590e-02,\n",
      "          1.6046e-02,  1.9584e-02, -3.1806e-02,  1.3155e-02, -1.9790e-02,\n",
      "          6.4101e-03,  4.7695e-02, -2.0484e-02, -2.9597e-02, -2.7234e-02,\n",
      "         -3.5520e-02,  1.3737e-02, -5.5384e-03, -1.0202e-02,  3.1716e-02,\n",
      "         -1.8368e-02,  3.6221e-02,  9.1044e-03,  2.0621e-02, -4.8905e-02,\n",
      "         -4.1324e-02, -4.9556e-02, -1.9178e-02]])\n",
      "edge_label_feat tensor([[-0.0831,  0.0137, -0.0534,  ..., -0.0151,  0.0360, -0.0788],\n",
      "        [-0.0831,  0.0064, -0.0570,  ..., -0.0115,  0.0297, -0.0718]])\n",
      "train_mask tensor([False,  True, False, False, False, False])\n",
      "val_mask tensor([False, False, False,  True, False, False])\n",
      "test_mask tensor([ True, False,  True, False,  True,  True])\n",
      "n_id tensor([   0, 2414, 1626, 1408, 1207, 1184])\n",
      "e_id tensor([0, 1, 2, 3, 4])\n",
      "num_sampled_nodes [1, 5]\n",
      "num_sampled_edges [5]\n",
      "input_id tensor([0])\n",
      "batch_size 1\n"
     ]
    }
   ],
   "source": [
    "for k, v in sg:\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2377, 2451, 3633, 4965, 9395])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row, col = cora.edge_index[0], cora.edge_index[1]\n",
    "\n",
    "np.where(row == 0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Collator:\n",
    "    def __init__(self, params):\n",
    "        self.params = params\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_pyg_dgl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
