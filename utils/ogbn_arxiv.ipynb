{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ogb.nodeproppred import PygNodePropPredDataset\n",
    "import os\n",
    "import os.path as osp\n",
    "import torch_geometric\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch_geometric.data import InMemoryDataset, download_url\n",
    "import random\n",
    "\n",
    "from encoder import SentenceEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the ogbn-arxiv dataset\n",
    "data_root = \"../data\"\n",
    "arxiv = PygNodePropPredDataset(name='ogbn-arxiv', root=data_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(num_nodes=169343, edge_index=[2, 1166243], x=[169343, 128], node_year=[169343, 1], y=[169343, 1])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = arxiv[0]\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_taxonomy(data_root):\n",
    "    # read categories and description file\n",
    "    f = open(osp.join(data_root, \"ogbn_arxiv\", \"arxiv_CS_categories.txt\"), \"r\").readlines()\n",
    "\n",
    "    state = 0\n",
    "    result = {\"id\": [], \"name\": [], \"description\": []}\n",
    "\n",
    "    for line in f:\n",
    "        if state == 0:\n",
    "            assert line.strip().startswith(\"cs.\")\n",
    "            category = (\"arxiv \"\n",
    "                + \" \".join(line.strip().split(\" \")[0].split(\".\")).lower())\n",
    "            # e.g. cs lo\n",
    "\n",
    "            name = line.strip()[7:-1]  # e. g. Logic in CS\n",
    "            result[\"id\"].append(category)\n",
    "            result[\"name\"].append(name)\n",
    "            state = 1\n",
    "            continue\n",
    "\n",
    "        elif state == 1:\n",
    "            description = line.strip()\n",
    "            result[\"description\"].append(description)\n",
    "            state = 2\n",
    "            continue\n",
    "\n",
    "        elif state == 2:\n",
    "            state = 0\n",
    "            continue\n",
    "\n",
    "    arxiv_cs_taxonomy = pd.DataFrame(result)\n",
    "    return arxiv_cs_taxonomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_feature(data_root):\n",
    "    nodeidx2paperid = pd.read_csv(osp.join(data_root, \"ogbn_arxiv/mapping/nodeidx2paperid.csv.gz\"), index_col=\"node idx\")\n",
    "\n",
    "    # Load the title and abstract of each paper\n",
    "    titleabs_url = \"https://snap.stanford.edu/ogb/data/misc/ogbn_arxiv/titleabs.tsv\"\n",
    "    titleabs_path = osp.join(data_root, \"ogbn_arxiv\", \"titleabs.tsv\")\n",
    "\n",
    "    if (osp.exists(titleabs_path)):\n",
    "        titleabs = pd.read_csv(titleabs_path, sep=\"\\t\", names=[\"paper id\", \"title\", \"abstract\"], index_col=\"paper id\")\n",
    "    else:\n",
    "        titleabs = pd.read_csv(titleabs_url, sep=\"\\t\", names=[\"paper id\", \"title\", \"abstract\"], index_col=\"paper id\")\n",
    "\n",
    "    titleabs = nodeidx2paperid.join(titleabs, on=\"paper id\")\n",
    "\n",
    "\n",
    "    # Prompt for the feature of nodes (can be changed accordingly)\n",
    "    node_feature_prompt = (\"Feature Node.\\n\"\n",
    "                        + \"Paper Title and Abstract : \"\n",
    "                        + titleabs[\"title\"]\n",
    "                        + \" + \"\n",
    "                        + titleabs[\"abstract\"])\n",
    "\n",
    "    node_feature_prompt_list = node_feature_prompt.values\n",
    "    return node_feature_prompt_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_feature(data_root):\n",
    "    arxiv_cs_taxonomy = get_taxonomy(data_root)\n",
    "\n",
    "    mapping_file = osp.join(data_root, \"ogbn_arxiv\", \"mapping\", \"labelidx2arxivcategeory.csv.gz\")\n",
    "    labelidx2arxivcategory = pd.read_csv(mapping_file)\n",
    "\n",
    "    arxiv_categ_vals = pd.merge(labelidx2arxivcategory, arxiv_cs_taxonomy, left_on=\"arxiv category\", right_on=\"id\")\n",
    "\n",
    "\n",
    "    # Prompt for the label nodes (can be changed accordingly)\n",
    "    label_node_prompt = (\"Prompt Node.\\n\"\n",
    "                        + \"Literature Category and Description: \"\n",
    "                        + arxiv_categ_vals[\"name\"]\n",
    "                        + \" + \"\n",
    "                        + arxiv_categ_vals[\"description\"])\n",
    "\n",
    "    label_node_prompt_list = label_node_prompt.values\n",
    "    return label_node_prompt_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArxivPyGDataset(InMemoryDataset):\n",
    "    def __init__(self, dataRoot=\"../data\", custom_dataRoot=\"../custom_data\", sentence_encoder=None, transform=None, pre_transform=None, pre_filter=None):\n",
    "        self.data_root = dataRoot\n",
    "        self.custom_data_root = custom_dataRoot\n",
    "        self.sentence_encoder = sentence_encoder\n",
    "        self.custom_data_dir = osp.join(self.custom_data_root, f\"ogbn_arxiv_{self.sentence_encoder.name}\")\n",
    "\n",
    "        if not osp.exists(self.custom_data_dir):\n",
    "            os.makedirs(self.custom_data_dir)\n",
    "\n",
    "        super().__init__(self.custom_data_dir, transform, pre_transform, pre_filter)\n",
    "\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return []\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return [\"data.pt\", \"texts.pkl\"]\n",
    "\n",
    "    def text_to_embed(self, texts):\n",
    "        if self.sentence_encoder is None:\n",
    "            raise NotImplementedError(\"Sentence Encoder is not passed\")\n",
    "        if texts is None:\n",
    "            return None\n",
    "        else:\n",
    "            return self.sentence_encoder.encode(texts) # returns to self.device\n",
    "\n",
    "    def encode_texts(self, texts):\n",
    "        if isinstance(texts[0], str):\n",
    "            return self.text_to_embed(texts)\n",
    "        return [self.text_to_embed(t) for t in texts]\n",
    "\n",
    "    def generate_custom_data(self):\n",
    "        node_texts = get_node_feature(self.data_root).tolist()\n",
    "        label_texts = get_label_feature(self.data_root).tolist()\n",
    "\n",
    "        # Prompt for prompt node/edge and edge texts (can be changed accordingly)\n",
    "        edge_texts = [\"Feature Edge.\\n Citation\"]\n",
    "        prompt_texts = [\"Prompt Node.\\n Node Classification of Literature Category\"]\n",
    "        prompt_edge_texts = [\"Prompt Edge.\"]\n",
    "\n",
    "        return [node_texts, label_texts, edge_texts, prompt_texts, prompt_edge_texts]\n",
    "\n",
    "    def process(self):\n",
    "        arxiv_data = PygNodePropPredDataset(name=\"ogbn-arxiv\", root=self.data_root)\n",
    "        arxiv_data_list = arxiv_data._data\n",
    "        arxiv_data_list.y = arxiv_data_list.y.squeeze()  # to flatten the y tensor\n",
    "\n",
    "        texts = self.generate_custom_data()\n",
    "        texts_embed = self.encode_texts(texts)\n",
    "\n",
    "        torch.save(texts, self.processed_paths[1])\n",
    "\n",
    "        arxiv_data_list.x_text_feat = texts_embed[0] # node text feature\n",
    "        arxiv_data_list.label_text_feat = texts_embed[1] # label text feature\n",
    "        arxiv_data_list.edge_text_feat = texts_embed[2] # edge text feature\n",
    "        arxiv_data_list.prompt_text_feat = texts_embed[3] # prompt node text feature\n",
    "        arxiv_data_list.prompt_edge_feat = texts_embed[4] # prompt edge text feature\n",
    "\n",
    "        # get dataset split\n",
    "        split_idx = arxiv_data.get_idx_split()\n",
    "        train_idx, valid_idx, test_idx = split_idx[\"train\"], split_idx[\"valid\"], split_idx[\"test\"]\n",
    "        # Generate 10 different permutations of train/valid/test split\n",
    "        train_idx_list = [train_idx[torch.randperm(train_idx.shape[0])] for _ in range(10)]\n",
    "        valid_idx_list = [valid_idx[torch.randperm(valid_idx.shape[0])] for _ in range(10)]\n",
    "        test_idx_list = [test_idx[torch.randperm(test_idx.shape[0])] for _ in range(10)]\n",
    "\n",
    "        arxiv_data_list.train_masks = train_idx_list\n",
    "        arxiv_data_list.val_masks = valid_idx_list\n",
    "        arxiv_data_list.test_masks = test_idx_list\n",
    "\n",
    "        data, slices = self.collate([arxiv_data_list]) # Pass the data_list as a list\n",
    "\n",
    "        torch.save((data, slices), self.processed_paths[0])\n",
    "        print(\"Arxiv is processed. Saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "LMencoder = SentenceEncoder(root=\"../lang_models\", name=\"ST\", device=0)\n",
    "custom_arxiv = ArxivPyGDataset(dataRoot=data_root, sentence_encoder=LMencoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "LMencoder2 = SentenceEncoder(root=\"../lang_models\", name=\"roberta\", device=0)\n",
    "custom_arxiv2 = ArxivPyGDataset(dataRoot=data_root, sentence_encoder=LMencoder2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(num_nodes=169343, edge_index=[2, 1166243], x=[169343, 128], node_year=[169343, 1], y=[169343], x_text_feat=[169343, 768], label_text_feat=[40, 768], edge_text_feat=[1, 768], prompt_text_feat=[1, 768], prompt_edge_feat=[1, 768], train_masks=[10], val_masks=[10], test_masks=[10])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv = custom_arxiv._data\n",
    "arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_path = osp.join(data_root, \"ogbn_arxiv\", \"split\", \"time\")\n",
    "\n",
    "train_path = osp.join(split_path, \"train.csv.gz\")\n",
    "valid_path = osp.join(split_path, \"valid.csv.gz\")\n",
    "test_path = osp.join(split_path, \"test.csv.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([     0,      1,      2, ..., 169145, 169148, 169251])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_idx = pd.read_csv(train_path, compression=\"gzip\", header=None).values.squeeze()\n",
    "train_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90941,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([     0,      1,      2,  ..., 169145, 169148, 169251],\n",
       "       dtype=torch.int32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_idx = torch.Tensor(train_idx).type(torch.int)\n",
    "train_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([ 75725,  96878, 143774,  ...,  56262, 167085,  14710],\n",
       "        dtype=torch.int32),\n",
       " tensor([ 80301,  74643, 135429,  ...,  68080, 157601,  83789],\n",
       "        dtype=torch.int32),\n",
       " tensor([100187,  64022,  84793,  ..., 126675, 144475, 106046],\n",
       "        dtype=torch.int32),\n",
       " tensor([  6742, 107581, 163094,  ..., 161658,  57453, 150625],\n",
       "        dtype=torch.int32),\n",
       " tensor([125837, 125939,  67942,  ...,  18600, 155247, 137965],\n",
       "        dtype=torch.int32),\n",
       " tensor([ 22988,  35298, 157988,  ...,  69873,  40173,  39958],\n",
       "        dtype=torch.int32),\n",
       " tensor([  9247, 164880, 162933,  ..., 118699, 153956, 157571],\n",
       "        dtype=torch.int32),\n",
       " tensor([116904, 128611, 125916,  ...,  20010,   1066,  99394],\n",
       "        dtype=torch.int32),\n",
       " tensor([  6099,  53158,  60685,  ...,  31903, 134991, 130488],\n",
       "        dtype=torch.int32),\n",
       " tensor([ 62644,  33167,  98725,  ...,  96238, 152349,  98719],\n",
       "        dtype=torch.int32)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_idx_list = [train_idx[torch.randperm(train_idx.shape[0])] for _ in range(10)]\n",
    "train_idx_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(num_nodes=169343, edge_index=[2, 1166243], x=[169343, 128], node_year=[169343, 1], y=[169343], x_text_feat=[169343, 768], label_text_feat=[40, 768], edge_text_feat=[1, 768], prompt_text_feat=[1, 768], prompt_edge_feat=[1, 768], train_masks=[10], val_masks=[10], test_masks=[10])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv_gpu = arxiv.to(\"cuda:2\")\n",
    "arxiv_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0579, -0.0525, -0.0726,  ...,  0.1734, -0.1728, -0.1401],\n",
      "        [-0.1245, -0.0707, -0.3252,  ...,  0.0685, -0.3721, -0.3010],\n",
      "        [-0.0802, -0.0233, -0.1838,  ...,  0.1099,  0.1176, -0.1399],\n",
      "        ...,\n",
      "        [-0.2205, -0.0366, -0.4022,  ...,  0.1134, -0.1614, -0.1452],\n",
      "        [-0.1382,  0.0409, -0.2518,  ..., -0.0893, -0.0413, -0.3761],\n",
      "        [-0.0299,  0.2684, -0.1611,  ...,  0.1208,  0.0776, -0.0910]],\n",
      "       device='cuda:2')\n",
      "tensor([[-0.0579, -0.0525, -0.0726,  ...,  0.1734, -0.1728, -0.1401],\n",
      "        [-0.1245, -0.0707, -0.3252,  ...,  0.0685, -0.3721, -0.3010],\n",
      "        [-0.0802, -0.0233, -0.1838,  ...,  0.1099,  0.1176, -0.1399],\n",
      "        ...,\n",
      "        [-0.2205, -0.0366, -0.4022,  ...,  0.1134, -0.1614, -0.1452],\n",
      "        [-0.1382,  0.0409, -0.2518,  ..., -0.0893, -0.0413, -0.3761],\n",
      "        [-0.0299,  0.2684, -0.1611,  ...,  0.1208,  0.0776, -0.0910]],\n",
      "       device='cuda:2')\n"
     ]
    }
   ],
   "source": [
    "print(arxiv.x)\n",
    "print(arxiv_gpu.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=2)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv_gpu.x_text_feat.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4,  5, 28,  ..., 10,  4,  1], device='cuda:2')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = arxiv.y.squeeze()\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PygNodePropPredDataset()"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arx = PygNodePropPredDataset(name=\"ogbn-arxiv\", root=data_root)\n",
    "arx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': tensor([     0,      1,      2,  ..., 169145, 169148, 169251]),\n",
       " 'valid': tensor([   349,    357,    366,  ..., 169185, 169261, 169296]),\n",
       " 'test': tensor([   346,    398,    451,  ..., 169340, 169341, 169342])}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "arx.get_idx_split()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_pyg_dgl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
