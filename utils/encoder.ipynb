{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import LlamaTokenizer, LlamaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceEncoder:\n",
    "    def __init__(self, name, root=\"../lang_models\", batch_size=512, device=0, multi_gpu=False):\n",
    "        self.name = name\n",
    "        self.root = root\n",
    "        self.batch_size = batch_size\n",
    "        self.multi_gpu = multi_gpu\n",
    "\n",
    "        # if device = 123 then use cpu otherwise use cuda\n",
    "        self.device = \"cpu\" if device==123 else f\"cuda:{device}\"\n",
    "\n",
    "        if self.name == \"ST\":\n",
    "            self.model = SentenceTransformer(\"sentence-transformers_multi-qa-distilbert-cos-v1\", device=self.device, cache_folder=root)\n",
    "\n",
    "        elif self.name == \"llama2\":\n",
    "            model_path = osp.join(self.root, \"llama-2-7b\")\n",
    "            self.tokenizer = LlamaTokenizer.from_pretrained(model_path, device=self.device)\n",
    "            self.model = LlamaModel.from_pretrained(model_path).to(self.device)\n",
    "\n",
    "        elif self.name == \"roberta\":\n",
    "            self.model = SentenceTransformer(\n",
    "                \"sentence-transformers/roberta-base-nli-stsb-mean-tokens\",\n",
    "                device=self.device,\n",
    "                cache_folder=root\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown language model: {name}.\")\n",
    "\n",
    "    def encode(self, texts):\n",
    "        if self.multi_gpu:\n",
    "            # Start the multi-process pool on all available CUDA devices\n",
    "            pool = self.model.start_multi_process_pool()\n",
    "            embeddings = self.model.encode_multi_process( texts, pool=pool, batch_size=self.batch_size)\n",
    "            embeddings = torch.from_numpy(embeddings)\n",
    "        else:\n",
    "            # return tensor instead of list of python integers\n",
    "            embeddings = self.model.encode(texts, batch_size=self.batch_size, show_progress_bar=True, convert_to_tensor=True)\n",
    "\n",
    "        # returns to self.device\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/prateek/graphs-with-llms-experiments/lang_models'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "osp.abspath(\"../lang_models/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_pyg_dgl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
